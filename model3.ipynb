{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 - Primarily on Text\n",
    "## POS tagged, Tokenized, Bigrams, Stemmed\n",
    "## LSTM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset\n",
    "data = pd.read_csv(\"E://Yelp//Unfiltered data//YelpZip//textonly\", header = None)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence Tokenization - Not applying back to the reviews at this phase - future steps\n",
    "tqdm.pandas()\n",
    "data[1].progress_apply(lambda txt: sent_tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Tokenization\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(\"Stopwords from the NLTK corpus: \", stop_words)\n",
    "\n",
    "def text_process(text):\n",
    "    #String punctuation provides all the necessary checks\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "tqdm.pandas()\n",
    "data[1] = data[1].progress_apply(lambda txt: text_process(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process took 6 hours 46 minutes - on last run - SAVE\n",
    "data.to_table(\"E://Yelp//Unfiltered data//YelpZip//text_mod1\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a column of sentiment polarity - averaged for the review text - might be useful during model building\n",
    "#Implemented here as it's easy to deal with the texts at this stage\n",
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_index(txt):\n",
    "    sent = 0\n",
    "    i = 0\n",
    "    for x in txt:\n",
    "        blob = TextBlob(x)\n",
    "        sent += blob.sentiment.polarity\n",
    "        i += 1\n",
    "    return sent/i\n",
    " \n",
    "data[4] = data[1].apply(lambda txt: sentiment_index(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem(txt):\n",
    "    words = []\n",
    "    for x in txt:\n",
    "        words.append(stemmer.stem(x))\n",
    "    return words\n",
    "\n",
    "data[1] = data[1].progress_apply(lambda txt: stem(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS tagging\n",
    "data[1] = data[1].progress_apply(lambda tokens: nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join back the list objects and their POS tags for CountVectorizer\n",
    "def join_back(txt):\n",
    "    new = ''\n",
    "    for x in txt:\n",
    "        new += '_'.join(x)+\" \"\n",
    "    new = new[:-1]\n",
    "    return new\n",
    "\n",
    "data[1] = data[1].progress_apply(lambda tokens: join_back(tokens))\n",
    "\n",
    "#Save Checkpoint\n",
    "data.to_table(\"E://Yelp//Unfiltered data//YelpZip//text_mod1\", header = None)\n",
    "\n",
    "#Document Term Matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#Tokenizing and Cleaning - again - satisfaction\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (2,2),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_counts, data[2], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decrease size of the sparse matrix\n",
    "#Feature engineering- sentiment index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need regularization\n",
    "#Cross validation?\n",
    "#Confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data on an LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determining the input shape. (Shape of dictionary)\n",
    "max_words = text_counts.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "embd_size=32\n",
    "vocab_size = max_words\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size, embd_size, input_length = max_words))\n",
    "model.add(LSTM(200))\n",
    "#Add dropout - Regularization\n",
    "#Add ensemble of engineered features\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "\n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
    "\n",
    "model.fit(X_train2, y_train2, validation_data = (X_valid, y_valid), batch_size = batch_size, epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print('Accuracy:', acc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add k fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
