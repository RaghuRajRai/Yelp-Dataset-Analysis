{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Model using Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "import random\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models.callbacks import PerplexityMetric\n",
    "from visdom import Visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(608458, 7)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r\"E:\\Yelp\\Unfiltered Data\\YelpZip\\Customs\\checkpoint\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The text data has already been cleaned. The stop words need to be removed though as we are not using TFIDF at this point for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    #Getting tokens from text\n",
    "    tokens = []\n",
    "    text_split = text.split()\n",
    "    for token in text_split:\n",
    "        tokens.append(token)\n",
    "    #Removing stop words from the list of tokens\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data split of both labels and develop a separate topic model for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real: (528019, 7) Fake: (80439, 7)\n"
     ]
    }
   ],
   "source": [
    "real = data[data['real'] == 1]\n",
    "fake = data[data['real'] == -1]\n",
    "print(\"Real:\",real.shape,\"Fake:\",fake.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a461b9622510473bb258c13d6f66a114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=608458), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Converting all our text to a list of preprocessed tokens\n",
    "review_data = []\n",
    "pbar = tqdm_notebook(total=data.shape[0])\n",
    "for text in data['review']:\n",
    "    tokens = preprocess_text(str(text))\n",
    "    review_data.append(tokens)\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528019 80439\n"
     ]
    }
   ],
   "source": [
    "#Separating real and fake reviews in different lists\n",
    "real = []\n",
    "fake = []\n",
    "\n",
    "for i in range(len(review_data)):\n",
    "    if data['real'][i] == 1:\n",
    "        real.append(review_data[i])\n",
    "    else:\n",
    "        fake.append(review_data[i])\n",
    "\n",
    "print(len(real), len(fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly creating a test and train set on both lists\n",
    "random.shuffle(real)\n",
    "random.shuffle(fake)\n",
    "\n",
    "test_size = 200\n",
    "\n",
    "real_train = real[200:]\n",
    "real_test = real[:200]\n",
    "fake_train = fake[200:]\n",
    "fake_test = fake[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of token: ['go', 'birthday', 'group', '13', 'us', 'take', 'reservation', 'pre', 'fix', 'menu', 'meat', 'cheese', 'plate', 'bread', 'salad', 'choice', 'different', '3', 'lasagna', 'dessert', 'drink', 'food', 'come', '60', 'person', 'affordable', 'portion', 'size', 'huge', 'food', 'deliciousand', 'ambiance', 'great', 'awesome', 'patio', 'garden', 'area', 'romantic', 'accommodate', 'big', 'group', 'definitely', 'would', 'suggest', 'place', 'especially', 'date'] || ['eat', 'alot', 'excellent', 'pizza', 'great', 'salad', 'great', 'atmospher', 'good', 'drink']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample of token:\",real_train[2], \"||\",fake_test[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dictionary of our data using Gensim and saving \n",
    "dictionary = corpora.Dictionary(review_data)\n",
    "dictionary.save('dictionary.gensim')\n",
    "\n",
    "#For saving corpus, use:\n",
    "#pickle.dump(corpus, open('corpus.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the perplexity score at the end of each epoch.\n",
    "#perplexity_logger = PerplexityMetric(corpus=common_corpus, logger='shell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 5\n",
    "\n",
    "#Getting 5 topics from real_train\n",
    "real_corpus = [dictionary.doc2bow(text) for text in real_train]\n",
    "perplexity1 = PerplexityMetric(corpus=real_corpus, logger='shell')\n",
    "real_model = gensim.models.ldamodel.LdaModel(real_corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15, callbacks=[perplexity1])\n",
    "real_topics = real_model.print_topics(num_words=7)\n",
    "for topic in real_topics:\n",
    "    print(topic)\n",
    "\n",
    "#Getting 5 topics from real_test\n",
    "fake_corpus = [dictionary.doc2bow(text) for text in fake_train]\n",
    "perplexity2 = PerplexityMetric(corpus=fake_corpus, logger='shell')\n",
    "fake_model = gensim.models.ldamodel.LdaModel(fake_corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15, callbacks=[perplexity2])\n",
    "fake_topics = fake_model.print_topics(num_words=7)\n",
    "for topic in fake_topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving all the acquired data\n",
    "dictionary.save(r'E:\\Yelp\\Unfiltered Data\\YelpZip\\models and stuff\\dictionary.gensim')\n",
    "pickle.dump(real_corpus, open(r'E:\\Yelp\\Unfiltered Data\\YelpZip\\models and stuff\\real_corpus.pkl', 'wb'))\n",
    "pickle.dump(fake_corpus, open(r'E:\\Yelp\\Unfiltered Data\\YelpZip\\models and stuff\\fake_corpus.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.gensim.prepare(real_model, real_corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.gensim.prepare(fake_model, fake_corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (95, 1), (156, 1), (497, 1), (710, 1), (879, 1), (1028, 1)]\n",
      "[(0, 0.02237967), (1, 0.022913761), (2, 0.22595412), (3, 0.37218377), (4, 0.35656866)]\n",
      "[(0, 0.20541026), (1, 0.22215351), (2, 0.5270318), (3, 0.02258687), (4, 0.022817561)]\n",
      "0.022379676\n"
     ]
    }
   ],
   "source": [
    "#Fitting a document\n",
    "new_doc = 'absolutely hated the place it suck bad waiter coffee smell bad'\n",
    "new_doc = preprocess_text(new_doc)\n",
    "new_doc_bow = dictionary.doc2bow(new_doc)\n",
    "print(new_doc_bow)\n",
    "print(real_model.get_document_topics(new_doc_bow))\n",
    "print(fake_model.get_document_topics(new_doc_bow))\n",
    "print(real_model.get_document_topics(new_doc_bow)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we create a function to evaluate the 400 test sets.\n",
    "### We joing the test sets in order: real_test, fake_test\n",
    "### We create y_actual with first 200 elements as 1 and next 200 as -1\n",
    "### We pass each individual text through all the 10 topics(5 real then 5 fake).\n",
    "### We find the best fit and estimate from the first 5 models(real) and then the best fit and estimate from the next 5 models(fake).\n",
    "### We compare the fits and accordingly assign the prediction to our text. \n",
    "### Append the prediction(1,-1) to y_pred list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the test set\n",
    "test_set = []\n",
    "test_set.append(real_test)\n",
    "test_set.append(fake_test)\n",
    "\n",
    "y_actual = []\n",
    "for i in range(400):\n",
    "    if i<200:\n",
    "        y_actual.append(1)\n",
    "    else:\n",
    "        y_actual.append(-1)\n",
    "        \n",
    "print(\"Size of test_set:\", len(test_set))\n",
    "print(\"y_actual:\", y_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions based on topics\n",
    "y_pred = []\n",
    "\n",
    "for test_text in test_set:\n",
    "    test_doc_bow = dictionary.doc2bow(test_text)\n",
    "    real_scores = []\n",
    "    fake_scores = []\n",
    "    for i in range(5):\n",
    "        #real_model.get_document_topics(new_doc_bow)[0][1] -> Fit value of doc on topic 0\n",
    "        real_scores.append(real_model.get_document_topics(new_doc_bow)[i][1])\n",
    "        fake_scores.append(fake_model.get_document_topics(new_doc_bow)[i][1])\n",
    "    \n",
    "    #Best Fit on real model\n",
    "    real_fit = max(real_scores)\n",
    "    #Best Fit on fake model\n",
    "    fake_fit = max(fake_scores)\n",
    "    \n",
    "    if real_fit >= fake_fit:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(-1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating\n",
    "real_correct = 0\n",
    "fake_correct = 0\n",
    "\n",
    "for i in range(400):\n",
    "    if y_pred[i] == y_actual[i]:\n",
    "        if i<200:\n",
    "            real_correct = real_correct + 1\n",
    "        else:\n",
    "            fake_correct = fake_correct + 1\n",
    "        \n",
    "acc = ((real_correct+fake_correct)/400)*100\n",
    "print(\"Overall Accuracy:\", acc,\"%\")\n",
    "print(\"Real prediction accuracy:\", ((real_correct/200)*100))\n",
    "print(\"Fake prediction accuracy:\", ((fake_correct/200)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
