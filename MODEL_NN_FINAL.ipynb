{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Final Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import keras\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Sequential\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"E:\\Yelp\\Unfiltered Data\\YelpZip\\Customs\\textonly\", header = None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(data.groupby(5).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['index', 'user', 'item', 'review', 'rating', 'real', 'date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first step is to remove stop words, but considering that we would be using tf-idf on our sparse matrix, we need not specifically remove the stop words as tf-idf would ensure that stop words don't get any more weightage. We will also be doing selective drops on the matrix. \n",
    "## This is to avoid the 6-7 hour hassle of processing time for removing stop words on a dataset of this size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctuations - Example\n",
    "test = \"This is a test-sentence to remove stop-word's!!! . . .\"\n",
    "trans = str.maketrans('', '', string.punctuation)\n",
    "words = test.split()\n",
    "print(words)\n",
    "print(string.punctuation)\n",
    "\n",
    "stripped = [w.translate(trans) for w in words]\n",
    "print(stripped)\n",
    "#We will also drop the empty cells\n",
    "# for word in stripped:\n",
    "#     if word == \"\":\n",
    "#         stripped.remove(word)\n",
    "a = [x for x in stripped if x != \"\"]\n",
    "print(a)\n",
    "b = [word.lower() for word in a]\n",
    "print(b)\n",
    "b = ' '.join(b)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    trans = str.maketrans('', '', string.punctuation)\n",
    "    words = text.split()\n",
    "    stripped = [word.translate(trans) for word in words]\n",
    "    a = [x for x in stripped if x!=\"\"]\n",
    "    b = [word.lower() for word in a]\n",
    "    b = ' '.join(b)\n",
    "    return b\n",
    "\n",
    "data['review'] = data['review'].progress_apply(lambda text: clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save checkpoint\n",
    "data.to_csv(r\"E:\\Yelp\\Unfiltered Data\\YelpZip\\Customs\\checkpoint\", header=True, index=False)\n",
    "\n",
    "#Use if kernel crashes\n",
    "#data = pd.read_csv(r\"E:\\Yelp\\Unfiltered Data\\YelpZip\\Customs\\checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming/Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#print(lemmatizer.lemmatize(\"going\"))\n",
    "\n",
    "def nltk2wn_tag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
    "\n",
    "    res_words = []\n",
    "    for word, tag in wn_tagged:\n",
    "        if tag is None:            \n",
    "            res_words.append(word)\n",
    "        else:\n",
    "            res_words.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(res_words)\n",
    "\n",
    "# test =  \"You better lose yourself in the music, the moment You own it, you better never let it go You only get one shot, do not miss your chance to blow This opportunity comes once in a lifetime\"\n",
    "# print(lemmatize_sentence(test))\n",
    "\n",
    "data['review'] = data['review'].progress_apply(lambda text: lemmatize_sentence(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save checkpoint\n",
    "# data.to_csv(r\"E:\\Yelp\\Unfiltered Data\\YelpZip\\Customs\\checkpoint\", header=True, index=False)\n",
    "\n",
    "#Use if kernel crashes\n",
    "data = pd.read_csv(r\"E:\\Yelp\\Unfiltered Data\\YelpZip\\Customs\\checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not adding POS tags to the documents cause we used POS tags to transform each world to it's lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df = 0.9, min_df = 20, ngram_range = (1,1), max_features = 20000)\n",
    "vectorizer.fit(data['review'].astype(str))\n",
    "vector = vectorizer.transform(data['review'].astype(str))\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(vector, data['real'], test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(keras.layers.Embedding(20000, 32, input_length=vector.shape[1]))\n",
    "model.add(keras.layers.LSTM(500, dropout=0.2, recurrent_dropout=0.2, activation='sigmoid'))\n",
    "# model.add(keras.layers.LSTM(512, dropout=0.2, recurrent_dropout=0.2, activation='sigmoid'))\n",
    "# model.add(keras.layers.Dense(64, activation='sigmoid'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_split=0.20, batch_size = 10, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save(r\"E:\\Yelp\\Unfiltered Data\\YelpZip\\Customs\\checkpoint\\my_model.h5\")  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", scores[1]*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other option - Using Keras text preprocessing - cause the one above wasn't working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1232c87e07b649b6be54b17f1e6c4f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=608458), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def binarize(x):\n",
    "    if x == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "data['real'] = data['real'].progress_apply(lambda x: binarize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words= 20000)\n",
    "tokenizer.fit_on_texts(data['review'].astype(str))\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(data['review'].astype(str))\n",
    "data1 = pad_sequences(sequences, maxlen=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data1, data['real'], test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 64)           1280000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1024)              4460544   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 5,773,377\n",
      "Trainable params: 5,773,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 64, input_length=150))\n",
    "model.add(LSTM(1024, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(32, activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 389412 samples, validate on 97354 samples\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_split=0.20, batch_size = 1000, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save(r\"E:\\Yelp\\Unfiltered Data\\YelpZip\\Customs\\checkpoint\\my_model.h5\")  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", scores[1]*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
