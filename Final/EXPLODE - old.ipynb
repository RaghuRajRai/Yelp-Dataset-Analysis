{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import keras\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Sequential\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, TimeDistributed, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import optimizers\n",
    "from keras.layers import Input\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>real</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5044</td>\n",
       "      <td>0</td>\n",
       "      <td>drink be bad the hot chocolate be water down a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2014-11-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5045</td>\n",
       "      <td>0</td>\n",
       "      <td>this be the bad experience ive ever have a cas...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2014-09-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5046</td>\n",
       "      <td>0</td>\n",
       "      <td>this be locate on the site of the old spruce s...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2013-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5047</td>\n",
       "      <td>0</td>\n",
       "      <td>i enjoy coffee and breakfast twice at toast du...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2014-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5048</td>\n",
       "      <td>0</td>\n",
       "      <td>i love toast the food choice be fantastic i lo...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2014-08-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  user  item                                             review  \\\n",
       "0      0  5044     0  drink be bad the hot chocolate be water down a...   \n",
       "1      1  5045     0  this be the bad experience ive ever have a cas...   \n",
       "2      2  5046     0  this be locate on the site of the old spruce s...   \n",
       "3      3  5047     0  i enjoy coffee and breakfast twice at toast du...   \n",
       "4      4  5048     0  i love toast the food choice be fantastic i lo...   \n",
       "\n",
       "   rating  real        date  \n",
       "0     1.0    -1  2014-11-16  \n",
       "1     1.0    -1  2014-09-08  \n",
       "2     3.0    -1  2013-10-06  \n",
       "3     5.0    -1  2014-11-30  \n",
       "4     5.0    -1  2014-08-28  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save checkpoint\n",
    "# data.to_csv(r\"E:\\Yelp\\Unfiltered Data\\YelpZip\\Customs\\checkpoint\", header=True, index=False)\n",
    "\n",
    "#Use if kernel crashes\n",
    "data = pd.read_csv(r\"E:\\Yelp\\Unfiltered Data\\YelpZip\\Customs\\checkpoint\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review'] = data['review'].apply(lambda s: str(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 for good\n",
    "#0 for bad\n",
    "def binarize(n):\n",
    "    if n==-1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data['real'] = data['real'].apply(lambda s: binarize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['review'], data['real'], test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>528019</td>\n",
       "      <td>528019</td>\n",
       "      <td>528019</td>\n",
       "      <td>528019</td>\n",
       "      <td>528019</td>\n",
       "      <td>528019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80439</td>\n",
       "      <td>80439</td>\n",
       "      <td>80439</td>\n",
       "      <td>80439</td>\n",
       "      <td>80439</td>\n",
       "      <td>80439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index    user    item  review  rating    date\n",
       "real                                                \n",
       "0     528019  528019  528019  528019  528019  528019\n",
       "1      80439   80439   80439   80439   80439   80439"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('real').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words= 2000)\n",
    "tokenizer.fit_on_texts(data['review'])\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "data1 = pad_sequences(sequences, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 100, 32)           64000     \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 100, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 50, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 25, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               205056    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 308,289\n",
      "Trainable params: 308,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# inputs = Input(shape=[100])\n",
    "# layers = Embedding(2000,32,input_length=100)(inputs)\n",
    "# layers = LSTM(128, recurrent_dropout=0.2)(layers)\n",
    "# layers = Dense(1)(layers)\n",
    "# layers = Activation('sigmoid')(layers)\n",
    "# model = Model(inputs=inputs,outputs=layers)\n",
    "# model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "\n",
    "# def create_conv_model():\n",
    "#     model_conv = Sequential()\n",
    "#     model_conv.add(Embedding(vocabulary_size, 100, input_length=50))\n",
    "#     model_conv.add(Dropout(0.2))\n",
    "#     model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "#     model_conv.add(MaxPooling1D(pool_size=4))\n",
    "#     model_conv.add(LSTM(100))\n",
    "#     model_conv.add(Dense(1, activation='sigmoid'))\n",
    "#     model_conv.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     return model_conv\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(2000, 32, input_length=100))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194000/425920 [============>.................] - ETA: 3:21:48 - loss: 0.7012 - acc: 0.13 - ETA: 3:11:34 - loss: 0.6558 - acc: 0.50 - ETA: 3:09:00 - loss: 0.5862 - acc: 0.62 - ETA: 3:07:28 - loss: 0.5248 - acc: 0.69 - ETA: 3:02:22 - loss: 0.4968 - acc: 0.72 - ETA: 3:01:36 - loss: 0.4768 - acc: 0.75 - ETA: 3:00:20 - loss: 0.4637 - acc: 0.76 - ETA: 2:59:14 - loss: 0.4515 - acc: 0.78 - ETA: 2:57:27 - loss: 0.4453 - acc: 0.78 - ETA: 2:55:46 - loss: 0.4341 - acc: 0.79 - ETA: 2:56:52 - loss: 0.4333 - acc: 0.80 - ETA: 2:55:42 - loss: 0.4301 - acc: 0.81 - ETA: 2:55:47 - loss: 0.4291 - acc: 0.81 - ETA: 2:55:38 - loss: 0.4265 - acc: 0.81 - ETA: 2:55:04 - loss: 0.4246 - acc: 0.82 - ETA: 2:55:50 - loss: 0.4213 - acc: 0.82 - ETA: 2:56:45 - loss: 0.4190 - acc: 0.82 - ETA: 2:55:35 - loss: 0.4167 - acc: 0.82 - ETA: 2:55:01 - loss: 0.4155 - acc: 0.83 - ETA: 2:54:25 - loss: 0.4159 - acc: 0.83 - ETA: 2:53:48 - loss: 0.4145 - acc: 0.83 - ETA: 2:53:58 - loss: 0.4140 - acc: 0.83 - ETA: 2:52:54 - loss: 0.4119 - acc: 0.83 - ETA: 2:52:38 - loss: 0.4113 - acc: 0.83 - ETA: 2:51:24 - loss: 0.4096 - acc: 0.83 - ETA: 2:51:02 - loss: 0.4097 - acc: 0.83 - ETA: 2:51:05 - loss: 0.4086 - acc: 0.84 - ETA: 2:50:37 - loss: 0.4071 - acc: 0.84 - ETA: 2:50:11 - loss: 0.4066 - acc: 0.84 - ETA: 2:49:45 - loss: 0.4047 - acc: 0.84 - ETA: 2:49:10 - loss: 0.4042 - acc: 0.84 - ETA: 2:49:02 - loss: 0.4032 - acc: 0.84 - ETA: 2:48:01 - loss: 0.4018 - acc: 0.84 - ETA: 2:48:02 - loss: 0.4008 - acc: 0.84 - ETA: 2:48:03 - loss: 0.3995 - acc: 0.84 - ETA: 2:47:23 - loss: 0.3989 - acc: 0.84 - ETA: 2:46:45 - loss: 0.3990 - acc: 0.84 - ETA: 2:46:02 - loss: 0.3981 - acc: 0.84 - ETA: 2:45:35 - loss: 0.3969 - acc: 0.85 - ETA: 2:45:16 - loss: 0.3963 - acc: 0.85 - ETA: 2:44:57 - loss: 0.3953 - acc: 0.85 - ETA: 2:44:13 - loss: 0.3953 - acc: 0.85 - ETA: 2:44:11 - loss: 0.3951 - acc: 0.85 - ETA: 2:43:58 - loss: 0.3952 - acc: 0.85 - ETA: 2:43:21 - loss: 0.3953 - acc: 0.85 - ETA: 2:42:52 - loss: 0.3956 - acc: 0.85 - ETA: 2:42:16 - loss: 0.3951 - acc: 0.85 - ETA: 2:41:57 - loss: 0.3949 - acc: 0.85 - ETA: 2:41:22 - loss: 0.3945 - acc: 0.85 - ETA: 2:41:06 - loss: 0.3930 - acc: 0.85 - ETA: 2:40:51 - loss: 0.3920 - acc: 0.85 - ETA: 2:40:40 - loss: 0.3917 - acc: 0.85 - ETA: 2:40:01 - loss: 0.3919 - acc: 0.85 - ETA: 2:39:45 - loss: 0.3915 - acc: 0.85 - ETA: 2:39:21 - loss: 0.3912 - acc: 0.85 - ETA: 2:38:56 - loss: 0.3911 - acc: 0.85 - ETA: 2:38:32 - loss: 0.3906 - acc: 0.85 - ETA: 2:38:07 - loss: 0.3898 - acc: 0.85 - ETA: 2:37:53 - loss: 0.3901 - acc: 0.85 - ETA: 2:37:22 - loss: 0.3896 - acc: 0.85 - ETA: 2:37:06 - loss: 0.3888 - acc: 0.85 - ETA: 2:36:48 - loss: 0.3887 - acc: 0.85 - ETA: 2:36:18 - loss: 0.3885 - acc: 0.85 - ETA: 2:36:08 - loss: 0.3881 - acc: 0.85 - ETA: 2:35:32 - loss: 0.3869 - acc: 0.85 - ETA: 2:35:02 - loss: 0.3868 - acc: 0.85 - ETA: 2:34:37 - loss: 0.3870 - acc: 0.85 - ETA: 2:34:09 - loss: 0.3868 - acc: 0.85 - ETA: 2:33:30 - loss: 0.3864 - acc: 0.85 - ETA: 2:33:00 - loss: 0.3856 - acc: 0.85 - ETA: 2:32:46 - loss: 0.3849 - acc: 0.85 - ETA: 2:32:09 - loss: 0.3846 - acc: 0.85 - ETA: 2:31:40 - loss: 0.3844 - acc: 0.85 - ETA: 2:31:13 - loss: 0.3841 - acc: 0.85 - ETA: 2:30:54 - loss: 0.3838 - acc: 0.85 - ETA: 2:30:20 - loss: 0.3832 - acc: 0.85 - ETA: 2:29:53 - loss: 0.3832 - acc: 0.85 - ETA: 2:29:23 - loss: 0.3832 - acc: 0.86 - ETA: 2:29:01 - loss: 0.3825 - acc: 0.86 - ETA: 2:28:30 - loss: 0.3822 - acc: 0.86 - ETA: 2:28:15 - loss: 0.3821 - acc: 0.86 - ETA: 2:27:50 - loss: 0.3821 - acc: 0.86 - ETA: 2:27:24 - loss: 0.3819 - acc: 0.86 - ETA: 2:26:52 - loss: 0.3824 - acc: 0.86 - ETA: 2:26:26 - loss: 0.3825 - acc: 0.86 - ETA: 2:25:59 - loss: 0.3822 - acc: 0.86 - ETA: 2:25:31 - loss: 0.3815 - acc: 0.86 - ETA: 2:25:03 - loss: 0.3816 - acc: 0.86 - ETA: 2:24:47 - loss: 0.3817 - acc: 0.86 - ETA: 2:24:18 - loss: 0.3818 - acc: 0.86 - ETA: 2:23:52 - loss: 0.3814 - acc: 0.86 - ETA: 2:23:33 - loss: 0.3809 - acc: 0.86 - ETA: 2:23:06 - loss: 0.3808 - acc: 0.86 - ETA: 2:22:47 - loss: 0.3803 - acc: 0.86 - ETA: 2:22:26 - loss: 0.3803 - acc: 0.86 - ETA: 2:22:00 - loss: 0.3800 - acc: 0.86 - ETA: 2:21:40 - loss: 0.3799 - acc: 0.86 - ETA: 2:21:08 - loss: 0.3799 - acc: 0.86 - ETA: 2:20:44 - loss: 0.3799 - acc: 0.86 - ETA: 2:20:19 - loss: 0.3796 - acc: 0.86 - ETA: 2:20:04 - loss: 0.3794 - acc: 0.86 - ETA: 2:19:45 - loss: 0.3791 - acc: 0.86 - ETA: 2:19:23 - loss: 0.3789 - acc: 0.86 - ETA: 2:18:52 - loss: 0.3788 - acc: 0.86 - ETA: 2:18:25 - loss: 0.3786 - acc: 0.86 - ETA: 2:17:56 - loss: 0.3783 - acc: 0.86 - ETA: 2:17:32 - loss: 0.3783 - acc: 0.86 - ETA: 2:17:11 - loss: 0.3783 - acc: 0.86 - ETA: 2:16:38 - loss: 0.3783 - acc: 0.86 - ETA: 2:16:14 - loss: 0.3783 - acc: 0.86 - ETA: 2:15:52 - loss: 0.3780 - acc: 0.86 - ETA: 2:15:27 - loss: 0.3780 - acc: 0.86 - ETA: 2:14:55 - loss: 0.3778 - acc: 0.86 - ETA: 2:14:23 - loss: 0.3776 - acc: 0.86 - ETA: 2:13:58 - loss: 0.3775 - acc: 0.86 - ETA: 2:13:38 - loss: 0.3774 - acc: 0.86 - ETA: 2:13:08 - loss: 0.3772 - acc: 0.86 - ETA: 2:12:49 - loss: 0.3773 - acc: 0.86 - ETA: 2:12:27 - loss: 0.3771 - acc: 0.86 - ETA: 2:12:00 - loss: 0.3768 - acc: 0.86 - ETA: 2:11:36 - loss: 0.3766 - acc: 0.86 - ETA: 2:11:10 - loss: 0.3764 - acc: 0.86 - ETA: 2:10:43 - loss: 0.3762 - acc: 0.86 - ETA: 2:10:14 - loss: 0.3760 - acc: 0.86 - ETA: 2:09:48 - loss: 0.3756 - acc: 0.86 - ETA: 2:09:25 - loss: 0.3755 - acc: 0.86 - ETA: 2:08:58 - loss: 0.3754 - acc: 0.86 - ETA: 2:08:33 - loss: 0.3752 - acc: 0.86 - ETA: 2:08:04 - loss: 0.3750 - acc: 0.86 - ETA: 2:07:35 - loss: 0.3749 - acc: 0.86 - ETA: 2:07:13 - loss: 0.3748 - acc: 0.86 - ETA: 2:06:51 - loss: 0.3748 - acc: 0.86 - ETA: 2:06:25 - loss: 0.3751 - acc: 0.86 - ETA: 2:05:57 - loss: 0.3749 - acc: 0.86 - ETA: 2:05:30 - loss: 0.3748 - acc: 0.86 - ETA: 2:05:01 - loss: 0.3749 - acc: 0.86 - ETA: 2:04:41 - loss: 0.3749 - acc: 0.86 - ETA: 2:04:13 - loss: 0.3748 - acc: 0.86 - ETA: 2:03:52 - loss: 0.3747 - acc: 0.86 - ETA: 2:03:31 - loss: 0.3747 - acc: 0.86 - ETA: 2:03:06 - loss: 0.3746 - acc: 0.86 - ETA: 2:02:36 - loss: 0.3744 - acc: 0.86 - ETA: 2:02:09 - loss: 0.3746 - acc: 0.86 - ETA: 2:01:47 - loss: 0.3744 - acc: 0.86 - ETA: 2:01:26 - loss: 0.3743 - acc: 0.86 - ETA: 2:00:58 - loss: 0.3742 - acc: 0.86 - ETA: 2:00:30 - loss: 0.3741 - acc: 0.86 - ETA: 1:59:59 - loss: 0.3740 - acc: 0.86 - ETA: 1:59:32 - loss: 0.3740 - acc: 0.86 - ETA: 1:59:08 - loss: 0.3740 - acc: 0.86 - ETA: 1:58:47 - loss: 0.3739 - acc: 0.86 - ETA: 1:58:27 - loss: 0.3738 - acc: 0.86 - ETA: 1:58:01 - loss: 0.3737 - acc: 0.86 - ETA: 1:57:33 - loss: 0.3734 - acc: 0.86 - ETA: 1:57:05 - loss: 0.3732 - acc: 0.86 - ETA: 1:56:40 - loss: 0.3733 - acc: 0.86 - ETA: 1:56:11 - loss: 0.3733 - acc: 0.86 - ETA: 1:55:46 - loss: 0.3729 - acc: 0.86 - ETA: 1:55:20 - loss: 0.3728 - acc: 0.86 - ETA: 1:54:57 - loss: 0.3726 - acc: 0.86 - ETA: 1:54:30 - loss: 0.3724 - acc: 0.86 - ETA: 1:54:06 - loss: 0.3723 - acc: 0.86 - ETA: 1:53:41 - loss: 0.3723 - acc: 0.86 - ETA: 1:53:12 - loss: 0.3722 - acc: 0.86 - ETA: 1:52:46 - loss: 0.3722 - acc: 0.86 - ETA: 1:52:18 - loss: 0.3722 - acc: 0.86 - ETA: 1:51:55 - loss: 0.3722 - acc: 0.86 - ETA: 1:51:33 - loss: 0.3722 - acc: 0.86 - ETA: 1:51:07 - loss: 0.3721 - acc: 0.86 - ETA: 1:50:44 - loss: 0.3719 - acc: 0.86 - ETA: 1:50:18 - loss: 0.3718 - acc: 0.86 - ETA: 1:49:57 - loss: 0.3717 - acc: 0.86 - ETA: 1:49:35 - loss: 0.3716 - acc: 0.86 - ETA: 1:49:06 - loss: 0.3716 - acc: 0.86 - ETA: 1:48:42 - loss: 0.3715 - acc: 0.86 - ETA: 1:48:15 - loss: 0.3714 - acc: 0.86 - ETA: 1:47:50 - loss: 0.3713 - acc: 0.86 - ETA: 1:47:23 - loss: 0.3711 - acc: 0.86 - ETA: 1:46:55 - loss: 0.3711 - acc: 0.86 - ETA: 1:46:30 - loss: 0.3711 - acc: 0.86 - ETA: 1:46:02 - loss: 0.3710 - acc: 0.86 - ETA: 1:45:41 - loss: 0.3711 - acc: 0.86 - ETA: 1:45:17 - loss: 0.3710 - acc: 0.86 - ETA: 1:44:48 - loss: 0.3711 - acc: 0.86 - ETA: 1:44:21 - loss: 0.3712 - acc: 0.86 - ETA: 1:43:57 - loss: 0.3710 - acc: 0.86 - ETA: 1:43:34 - loss: 0.3710 - acc: 0.86 - ETA: 1:43:06 - loss: 0.3709 - acc: 0.86 - ETA: 1:42:43 - loss: 0.3709 - acc: 0.86 - ETA: 1:42:17 - loss: 0.3709 - acc: 0.86 - ETA: 1:41:51 - loss: 0.3708 - acc: 0.86 - ETA: 1:41:24 - loss: 0.3707 - acc: 0.86 - ETA: 1:40:58 - loss: 0.3706 - acc: 0.86 - ETA: 1:40:33 - loss: 0.3706 - acc: 0.8648\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393000/425920 [==========================>...] - ETA: 1:40:06 - loss: 0.3705 - acc: 0.86 - ETA: 1:39:42 - loss: 0.3704 - acc: 0.86 - ETA: 1:39:14 - loss: 0.3704 - acc: 0.86 - ETA: 1:38:50 - loss: 0.3704 - acc: 0.86 - ETA: 1:38:23 - loss: 0.3703 - acc: 0.86 - ETA: 1:37:55 - loss: 0.3702 - acc: 0.86 - ETA: 1:37:30 - loss: 0.3701 - acc: 0.86 - ETA: 1:37:06 - loss: 0.3700 - acc: 0.86 - ETA: 1:36:39 - loss: 0.3700 - acc: 0.86 - ETA: 1:36:14 - loss: 0.3699 - acc: 0.86 - ETA: 1:35:47 - loss: 0.3699 - acc: 0.86 - ETA: 1:35:20 - loss: 0.3701 - acc: 0.86 - ETA: 1:34:56 - loss: 0.3703 - acc: 0.86 - ETA: 1:34:30 - loss: 0.3702 - acc: 0.86 - ETA: 1:34:05 - loss: 0.3702 - acc: 0.86 - ETA: 1:33:38 - loss: 0.3701 - acc: 0.86 - ETA: 1:33:11 - loss: 0.3700 - acc: 0.86 - ETA: 1:32:45 - loss: 0.3700 - acc: 0.86 - ETA: 1:32:20 - loss: 0.3700 - acc: 0.86 - ETA: 1:31:55 - loss: 0.3700 - acc: 0.86 - ETA: 1:31:28 - loss: 0.3700 - acc: 0.86 - ETA: 1:31:02 - loss: 0.3700 - acc: 0.86 - ETA: 1:30:34 - loss: 0.3700 - acc: 0.86 - ETA: 1:30:08 - loss: 0.3699 - acc: 0.86 - ETA: 1:29:43 - loss: 0.3698 - acc: 0.86 - ETA: 1:29:16 - loss: 0.3697 - acc: 0.86 - ETA: 1:28:50 - loss: 0.3696 - acc: 0.86 - ETA: 1:28:22 - loss: 0.3697 - acc: 0.86 - ETA: 1:27:53 - loss: 0.3698 - acc: 0.86 - ETA: 1:27:27 - loss: 0.3699 - acc: 0.86 - ETA: 1:27:01 - loss: 0.3697 - acc: 0.86 - ETA: 1:26:34 - loss: 0.3697 - acc: 0.86 - ETA: 1:26:07 - loss: 0.3697 - acc: 0.86 - ETA: 1:25:41 - loss: 0.3696 - acc: 0.86 - ETA: 1:25:14 - loss: 0.3696 - acc: 0.86 - ETA: 1:24:50 - loss: 0.3696 - acc: 0.86 - ETA: 1:24:27 - loss: 0.3695 - acc: 0.86 - ETA: 1:23:59 - loss: 0.3695 - acc: 0.86 - ETA: 1:23:33 - loss: 0.3695 - acc: 0.86 - ETA: 1:23:05 - loss: 0.3696 - acc: 0.86 - ETA: 1:22:39 - loss: 0.3695 - acc: 0.86 - ETA: 1:22:14 - loss: 0.3694 - acc: 0.86 - ETA: 1:21:50 - loss: 0.3693 - acc: 0.86 - ETA: 1:21:22 - loss: 0.3691 - acc: 0.86 - ETA: 1:20:56 - loss: 0.3692 - acc: 0.86 - ETA: 1:20:29 - loss: 0.3692 - acc: 0.86 - ETA: 1:20:03 - loss: 0.3691 - acc: 0.86 - ETA: 1:19:38 - loss: 0.3690 - acc: 0.86 - ETA: 1:19:14 - loss: 0.3689 - acc: 0.86 - ETA: 1:18:48 - loss: 0.3687 - acc: 0.86 - ETA: 1:18:20 - loss: 0.3687 - acc: 0.86 - ETA: 1:17:54 - loss: 0.3686 - acc: 0.86 - ETA: 1:17:29 - loss: 0.3685 - acc: 0.86 - ETA: 1:17:05 - loss: 0.3684 - acc: 0.86 - ETA: 1:16:38 - loss: 0.3682 - acc: 0.86 - ETA: 1:16:13 - loss: 0.3682 - acc: 0.86 - ETA: 1:15:46 - loss: 0.3680 - acc: 0.86 - ETA: 1:15:21 - loss: 0.3679 - acc: 0.86 - ETA: 1:14:54 - loss: 0.3678 - acc: 0.86 - ETA: 1:14:29 - loss: 0.3677 - acc: 0.86 - ETA: 1:14:03 - loss: 0.3678 - acc: 0.86 - ETA: 1:13:38 - loss: 0.3677 - acc: 0.86 - ETA: 1:13:12 - loss: 0.3678 - acc: 0.86 - ETA: 1:12:45 - loss: 0.3678 - acc: 0.86 - ETA: 1:12:19 - loss: 0.3677 - acc: 0.86 - ETA: 1:11:53 - loss: 0.3676 - acc: 0.86 - ETA: 1:11:27 - loss: 0.3675 - acc: 0.86 - ETA: 1:11:02 - loss: 0.3676 - acc: 0.86 - ETA: 1:10:36 - loss: 0.3676 - acc: 0.86 - ETA: 1:10:08 - loss: 0.3676 - acc: 0.86 - ETA: 1:09:40 - loss: 0.3677 - acc: 0.86 - ETA: 1:09:15 - loss: 0.3677 - acc: 0.86 - ETA: 1:08:52 - loss: 0.3677 - acc: 0.86 - ETA: 1:08:25 - loss: 0.3676 - acc: 0.86 - ETA: 1:08:01 - loss: 0.3677 - acc: 0.86 - ETA: 1:07:35 - loss: 0.3675 - acc: 0.86 - ETA: 1:07:08 - loss: 0.3675 - acc: 0.86 - ETA: 1:06:41 - loss: 0.3675 - acc: 0.86 - ETA: 1:06:15 - loss: 0.3673 - acc: 0.86 - ETA: 1:05:49 - loss: 0.3673 - acc: 0.86 - ETA: 1:05:22 - loss: 0.3673 - acc: 0.86 - ETA: 1:04:56 - loss: 0.3672 - acc: 0.86 - ETA: 1:04:32 - loss: 0.3671 - acc: 0.86 - ETA: 1:04:07 - loss: 0.3671 - acc: 0.86 - ETA: 1:03:42 - loss: 0.3671 - acc: 0.86 - ETA: 1:03:15 - loss: 0.3670 - acc: 0.86 - ETA: 1:02:50 - loss: 0.3669 - acc: 0.86 - ETA: 1:02:23 - loss: 0.3668 - acc: 0.86 - ETA: 1:01:57 - loss: 0.3668 - acc: 0.86 - ETA: 1:01:32 - loss: 0.3668 - acc: 0.86 - ETA: 1:01:06 - loss: 0.3668 - acc: 0.86 - ETA: 1:00:41 - loss: 0.3668 - acc: 0.86 - ETA: 1:00:16 - loss: 0.3668 - acc: 0.86 - ETA: 59:50 - loss: 0.3668 - acc: 0.8657 - ETA: 59:25 - loss: 0.3667 - acc: 0.86 - ETA: 58:58 - loss: 0.3666 - acc: 0.86 - ETA: 58:32 - loss: 0.3665 - acc: 0.86 - ETA: 58:05 - loss: 0.3666 - acc: 0.86 - ETA: 57:39 - loss: 0.3667 - acc: 0.86 - ETA: 57:12 - loss: 0.3666 - acc: 0.86 - ETA: 56:46 - loss: 0.3665 - acc: 0.86 - ETA: 56:18 - loss: 0.3665 - acc: 0.86 - ETA: 55:52 - loss: 0.3665 - acc: 0.86 - ETA: 55:27 - loss: 0.3663 - acc: 0.86 - ETA: 55:00 - loss: 0.3662 - acc: 0.86 - ETA: 54:33 - loss: 0.3660 - acc: 0.86 - ETA: 54:07 - loss: 0.3660 - acc: 0.86 - ETA: 53:40 - loss: 0.3661 - acc: 0.86 - ETA: 53:14 - loss: 0.3661 - acc: 0.86 - ETA: 52:48 - loss: 0.3662 - acc: 0.86 - ETA: 52:23 - loss: 0.3661 - acc: 0.86 - ETA: 51:57 - loss: 0.3660 - acc: 0.86 - ETA: 51:31 - loss: 0.3659 - acc: 0.86 - ETA: 51:06 - loss: 0.3659 - acc: 0.86 - ETA: 50:39 - loss: 0.3658 - acc: 0.86 - ETA: 50:14 - loss: 0.3657 - acc: 0.86 - ETA: 49:49 - loss: 0.3657 - acc: 0.86 - ETA: 49:22 - loss: 0.3656 - acc: 0.86 - ETA: 48:55 - loss: 0.3655 - acc: 0.86 - ETA: 48:30 - loss: 0.3654 - acc: 0.86 - ETA: 48:03 - loss: 0.3654 - acc: 0.86 - ETA: 47:39 - loss: 0.3654 - acc: 0.86 - ETA: 47:13 - loss: 0.3653 - acc: 0.86 - ETA: 46:47 - loss: 0.3652 - acc: 0.86 - ETA: 46:22 - loss: 0.3652 - acc: 0.86 - ETA: 45:57 - loss: 0.3651 - acc: 0.86 - ETA: 45:32 - loss: 0.3652 - acc: 0.86 - ETA: 45:06 - loss: 0.3652 - acc: 0.86 - ETA: 44:39 - loss: 0.3651 - acc: 0.86 - ETA: 44:13 - loss: 0.3652 - acc: 0.86 - ETA: 43:48 - loss: 0.3652 - acc: 0.86 - ETA: 43:22 - loss: 0.3652 - acc: 0.86 - ETA: 42:56 - loss: 0.3652 - acc: 0.86 - ETA: 42:30 - loss: 0.3651 - acc: 0.86 - ETA: 42:04 - loss: 0.3651 - acc: 0.86 - ETA: 41:38 - loss: 0.3650 - acc: 0.86 - ETA: 41:11 - loss: 0.3649 - acc: 0.86 - ETA: 40:46 - loss: 0.3649 - acc: 0.86 - ETA: 40:20 - loss: 0.3650 - acc: 0.86 - ETA: 39:53 - loss: 0.3650 - acc: 0.86 - ETA: 39:27 - loss: 0.3649 - acc: 0.86 - ETA: 39:01 - loss: 0.3649 - acc: 0.86 - ETA: 38:35 - loss: 0.3649 - acc: 0.86 - ETA: 38:09 - loss: 0.3649 - acc: 0.86 - ETA: 37:44 - loss: 0.3648 - acc: 0.86 - ETA: 37:17 - loss: 0.3648 - acc: 0.86 - ETA: 36:51 - loss: 0.3647 - acc: 0.86 - ETA: 36:25 - loss: 0.3647 - acc: 0.86 - ETA: 35:59 - loss: 0.3647 - acc: 0.86 - ETA: 35:33 - loss: 0.3647 - acc: 0.86 - ETA: 35:07 - loss: 0.3647 - acc: 0.86 - ETA: 34:41 - loss: 0.3647 - acc: 0.86 - ETA: 34:16 - loss: 0.3647 - acc: 0.86 - ETA: 33:50 - loss: 0.3646 - acc: 0.86 - ETA: 33:24 - loss: 0.3646 - acc: 0.86 - ETA: 32:58 - loss: 0.3647 - acc: 0.86 - ETA: 32:32 - loss: 0.3648 - acc: 0.86 - ETA: 32:06 - loss: 0.3648 - acc: 0.86 - ETA: 31:40 - loss: 0.3648 - acc: 0.86 - ETA: 31:14 - loss: 0.3647 - acc: 0.86 - ETA: 30:49 - loss: 0.3646 - acc: 0.86 - ETA: 30:22 - loss: 0.3646 - acc: 0.86 - ETA: 29:56 - loss: 0.3646 - acc: 0.86 - ETA: 29:30 - loss: 0.3646 - acc: 0.86 - ETA: 29:04 - loss: 0.3646 - acc: 0.86 - ETA: 28:38 - loss: 0.3645 - acc: 0.86 - ETA: 28:12 - loss: 0.3645 - acc: 0.86 - ETA: 27:46 - loss: 0.3645 - acc: 0.86 - ETA: 27:20 - loss: 0.3645 - acc: 0.86 - ETA: 26:53 - loss: 0.3645 - acc: 0.86 - ETA: 26:27 - loss: 0.3645 - acc: 0.86 - ETA: 26:01 - loss: 0.3645 - acc: 0.86 - ETA: 25:35 - loss: 0.3645 - acc: 0.86 - ETA: 25:09 - loss: 0.3645 - acc: 0.86 - ETA: 24:42 - loss: 0.3645 - acc: 0.86 - ETA: 24:16 - loss: 0.3645 - acc: 0.86 - ETA: 23:50 - loss: 0.3646 - acc: 0.86 - ETA: 23:25 - loss: 0.3645 - acc: 0.86 - ETA: 22:58 - loss: 0.3646 - acc: 0.86 - ETA: 22:32 - loss: 0.3646 - acc: 0.86 - ETA: 22:06 - loss: 0.3646 - acc: 0.86 - ETA: 21:40 - loss: 0.3646 - acc: 0.86 - ETA: 21:14 - loss: 0.3645 - acc: 0.86 - ETA: 20:49 - loss: 0.3644 - acc: 0.86 - ETA: 20:23 - loss: 0.3644 - acc: 0.86 - ETA: 19:57 - loss: 0.3643 - acc: 0.86 - ETA: 19:31 - loss: 0.3642 - acc: 0.86 - ETA: 19:05 - loss: 0.3641 - acc: 0.86 - ETA: 18:38 - loss: 0.3641 - acc: 0.86 - ETA: 18:12 - loss: 0.3642 - acc: 0.86 - ETA: 17:47 - loss: 0.3642 - acc: 0.86 - ETA: 17:21 - loss: 0.3642 - acc: 0.86 - ETA: 16:54 - loss: 0.3642 - acc: 0.86 - ETA: 16:28 - loss: 0.3642 - acc: 0.86 - ETA: 16:02 - loss: 0.3641 - acc: 0.86 - ETA: 15:36 - loss: 0.3642 - acc: 0.86 - ETA: 15:10 - loss: 0.3641 - acc: 0.86 - ETA: 14:44 - loss: 0.3640 - acc: 0.86 - ETA: 14:18 - loss: 0.3641 - acc: 0.8660"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425920/425920 [==============================] - ETA: 13:52 - loss: 0.3639 - acc: 0.86 - ETA: 13:26 - loss: 0.3639 - acc: 0.86 - ETA: 13:00 - loss: 0.3638 - acc: 0.86 - ETA: 12:34 - loss: 0.3638 - acc: 0.86 - ETA: 12:08 - loss: 0.3638 - acc: 0.86 - ETA: 11:42 - loss: 0.3638 - acc: 0.86 - ETA: 11:16 - loss: 0.3637 - acc: 0.86 - ETA: 10:49 - loss: 0.3637 - acc: 0.86 - ETA: 10:23 - loss: 0.3637 - acc: 0.86 - ETA: 9:57 - loss: 0.3638 - acc: 0.8662 - ETA: 9:31 - loss: 0.3638 - acc: 0.866 - ETA: 9:05 - loss: 0.3638 - acc: 0.866 - ETA: 8:39 - loss: 0.3639 - acc: 0.866 - ETA: 8:13 - loss: 0.3638 - acc: 0.866 - ETA: 7:47 - loss: 0.3638 - acc: 0.866 - ETA: 7:21 - loss: 0.3638 - acc: 0.866 - ETA: 6:55 - loss: 0.3638 - acc: 0.866 - ETA: 6:29 - loss: 0.3637 - acc: 0.866 - ETA: 6:02 - loss: 0.3637 - acc: 0.866 - ETA: 5:36 - loss: 0.3637 - acc: 0.866 - ETA: 5:10 - loss: 0.3637 - acc: 0.866 - ETA: 4:44 - loss: 0.3636 - acc: 0.866 - ETA: 4:18 - loss: 0.3637 - acc: 0.866 - ETA: 3:52 - loss: 0.3637 - acc: 0.866 - ETA: 3:26 - loss: 0.3637 - acc: 0.866 - ETA: 3:00 - loss: 0.3637 - acc: 0.866 - ETA: 2:34 - loss: 0.3637 - acc: 0.866 - ETA: 2:08 - loss: 0.3638 - acc: 0.866 - ETA: 1:42 - loss: 0.3638 - acc: 0.866 - ETA: 1:16 - loss: 0.3637 - acc: 0.866 - ETA: 50s - loss: 0.3638 - acc: 0.866 - ETA: 23s - loss: 0.3638 - acc: 0.86 - 11105s 26ms/step - loss: 0.3637 - acc: 0.8660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f5c4f7a5c0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data1,y_train,batch_size=1000,epochs=1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = pad_sequences(test_sequences,maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accr = model.evaluate(test_sequences_matrix,y_test)\n",
    "y_pred = model.predict(test_sequences_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.91      0.90    158422\n",
      "          1       0.34      0.30      0.31     24116\n",
      "\n",
      "avg / total       0.82      0.83      0.83    182538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tst = []\n",
    "for i in y_pred:\n",
    "    if i >= 0.20:\n",
    "        tst.append(1)\n",
    "    else:\n",
    "        tst.append(0)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182538/182538 [==============================] - ETA: 51 - ETA: 34 - ETA: 28 - ETA: 26 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 21s 116us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_sequences_matrix, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.81260888150369 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", results[1]*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14800052828693527"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
