{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Model using Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "import random\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models.callbacks import PerplexityMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(608458, 7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r\"E:\\Yelp\\Unfiltered Data\\YelpZip\\Customs\\checkpoint\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The text data has already been cleaned. The stop words need to be removed though as we are not using TFIDF at this point for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    #Getting tokens from text\n",
    "    tokens = []\n",
    "    text_split = text.split()\n",
    "    for token in text_split:\n",
    "        tokens.append(token)\n",
    "    #Removing stop words from the list of tokens\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data split of both labels and develop a separate topic model for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real: (528019, 7) Fake: (80439, 7)\n"
     ]
    }
   ],
   "source": [
    "real = data[data['real'] == 1]\n",
    "fake = data[data['real'] == -1]\n",
    "print(\"Real:\",real.shape,\"Fake:\",fake.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9034ebe547a54ea6a533f2154badf636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=608458), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Converting all our text to a list of preprocessed tokens\n",
    "review_data = []\n",
    "pbar = tqdm_notebook(total=data.shape[0])\n",
    "for text in data['review']:\n",
    "    tokens = preprocess_text(str(text))\n",
    "    review_data.append(tokens)\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528019 80439\n"
     ]
    }
   ],
   "source": [
    "#Separating real and fake reviews in different lists\n",
    "real = []\n",
    "fake = []\n",
    "\n",
    "for i in range(len(review_data)):\n",
    "    if data['real'][i] == 1:\n",
    "        real.append(review_data[i])\n",
    "    else:\n",
    "        fake.append(review_data[i])\n",
    "\n",
    "print(len(real), len(fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't use now\n",
    "#Randomly creating a test and train set on both lists\n",
    "random.shuffle(real)\n",
    "random.shuffle(fake)\n",
    "\n",
    "test_size = 200\n",
    "\n",
    "real_train = real[200:]\n",
    "real_test = real[:200]\n",
    "fake_train = fake[200:]\n",
    "fake_test = fake[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    i fell madly in love with these tiny sandwich ...\n",
       "1    my review will strictly be on what i order let...\n",
       "2    im from ny and i can say this pizza be delicio...\n",
       "3    i have never have coffee i enjoy in fact i thi...\n",
       "4    the mediterranean version of chipotle decor be...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.read_csv(r\"E:\\Yelp\\Unfiltered Data\\WorkingSet\\X_train\") \n",
    "X_train = X_train['review']\n",
    "X_test = pd.read_csv(r\"E:\\Yelp\\Unfiltered Data\\WorkingSet\\X_test\") \n",
    "X_test = X_test['review']\n",
    "Y_train= pd.read_csv(r\"E:\\Yelp\\Unfiltered Data\\WorkingSet\\y_train\", header=None) \n",
    "Y_train = Y_train[1]\n",
    "Y_test= pd.read_csv(r\"E:\\Yelp\\Unfiltered Data\\WorkingSet\\y_test\", header=None) \n",
    "Y_test = Y_test[1]\n",
    "X_train.head()\n",
    "# index = []\n",
    "# for i in Y_train:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my review will strictly be on what i order let me start by say the texture for each macaron be phenomenal and how a macaron should be crispy thin on the exterior and beyond delicate on the interior these literally melt in your mouth salt caramel just the right amount of salt to make you salivate but unfortunately the caramel be cook a tad too long and leave an unpleasant bitterness in my mouth chocolate nothing spectacular the chocolate taste like midgrade chocolate there be nothing too luscious about the ganache rise not a fan of rise so that should say more than enough ice box seasonal refreshing but the flavor be so delicate it be almost nonexistent and leave more of a cool effect on the palette instead of an everlasting flavor coconut coconut can go in so many direction and the profile on this be more of that sunblock coconut i prefer a delicate young coconut flavor pistachio the best of the dozen true pistachio flavor lemon the instant it hit your palette you think you be eat a spoonful of overly sweet lemon curd then the curdness go away and youre leave with a lemon ganache bright tart and definitely a pick me up on the palette orange blossom while the orange flavor be nice and delicate the blossom ruin it for me again my palette be super sensitive to anything perfumey and flowerish coffee this be probably the bad one for me while i enjoy a good cup of jo the ganache be super gritty literally felt like i be eat espresso powder framboise nothing special just some jam sandwich between two almond meringue vanilla i enjoy the flavor and overall vanilla essence not too strong and not too subtle just enough to satisfy fruit jam almost identical to the framboise but with a strawberry profile overall again of the 934895723984 ive try in my lifetime there have the best and most true texture hand down however the degree of flavor didnt spark my slight interest like i always say you wont know unless you try it for yourself'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rangex = X_train.shape[0]\n",
    "rangex\n",
    "Y_train[2]\n",
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_indexes = []\n",
    "fake_indexes = []\n",
    "real_train = []\n",
    "fake_train = []\n",
    "\n",
    "for i in range(rangex):\n",
    "    if Y_train[i] == 1:\n",
    "        real_train.append(X_train[i])\n",
    "    else:\n",
    "        fake_train.append(X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad723eecb14344fc93748acf4b3e20a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=608458), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112c144ec2464b0aad0f11c25f7557a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=608458), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "real_t = []\n",
    "fake_t = []\n",
    "pbar = tqdm_notebook(total=data.shape[0])\n",
    "for text in real_train:\n",
    "    tokens = preprocess_text(str(text))\n",
    "    real_t.append(tokens)\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "real_train = real_t\n",
    "\n",
    "pbar = tqdm_notebook(total=data.shape[0])\n",
    "for text in fake_train:\n",
    "    tokens = preprocess_text(str(text))\n",
    "    fake_t.append(tokens)\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "fake_train = fake_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of token: ['never', 'coffee', 'enjoy', 'fact', 'think', 'hat', 'coffee', 'bean', 'format', 'drink', 'need', 'someone', 'would', 'need', 'medicine', 'latte', 'w', 'whole', 'milk', 'thimble', 'simple', 'syrup', 'comfort', 'honestly', 'nothing', 'ever', 'review', 'perfect', 'im', 'extremely', 'sensitive', 'service', 'spot', 'move', 'quickly', 'update', 'dilly', 'dally', 'man', 'state', 'around', 'drop', 'around', 'place'] || ['great', 'first', 'experience', 'excellent', 'service']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample of token:\",real_train[2], \"||\",fake_train[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = []\n",
    "for i in real_train:\n",
    "    review_data.append(i)\n",
    "for i in fake_train:\n",
    "    review_data.append(i)\n",
    "#Creating dictionary of our data using Gensim and saving \n",
    "dictionary = corpora.Dictionary(review_data)\n",
    "dictionary.save('dictionary.gensim')\n",
    "\n",
    "#For saving corpus, use:\n",
    "#pickle.dump(corpus, open('corpus.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='C:/Users/elonm/Desktop/gensim.log',format=\"%(asctime)s:%(levelname)s:%(message)s\",level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.016*\"good\" + 0.012*\"chicken\" + 0.011*\"place\" + 0.011*\"get\" + 0.011*\"pork\" + 0.010*\"food\" + 0.010*\"like\"')\n",
      "(1, '0.025*\"pizza\" + 0.011*\"place\" + 0.011*\"sandwich\" + 0.011*\"get\" + 0.010*\"good\" + 0.009*\"best\" + 0.009*\"like\"')\n",
      "(2, '0.010*\"dish\" + 0.009*\"good\" + 0.008*\"cheese\" + 0.008*\"order\" + 0.008*\"salad\" + 0.007*\"delicious\" + 0.007*\"dessert\"')\n",
      "(3, '0.028*\"food\" + 0.027*\"place\" + 0.027*\"great\" + 0.022*\"good\" + 0.014*\"go\" + 0.012*\"service\" + 0.010*\"love\"')\n",
      "(4, '0.014*\"get\" + 0.013*\"order\" + 0.012*\"food\" + 0.011*\"go\" + 0.011*\"us\" + 0.010*\"wait\" + 0.010*\"time\"')\n",
      "(0, '0.095*\"pizza\" + 0.017*\"pie\" + 0.011*\"crust\" + 0.011*\"slice\" + 0.006*\"thin\" + 0.005*\"pat\" + 0.004*\"cheesesteak\"')\n",
      "(1, '0.011*\"order\" + 0.011*\"good\" + 0.011*\"chicken\" + 0.010*\"sauce\" + 0.010*\"cheese\" + 0.009*\"like\" + 0.009*\"taste\"')\n",
      "(2, '0.006*\"die\" + 0.005*\"das\" + 0.004*\"da\" + 0.004*\"de\" + 0.004*\"der\" + 0.004*\"hat\" + 0.003*\"zu\"')\n",
      "(3, '0.014*\"go\" + 0.013*\"food\" + 0.012*\"get\" + 0.011*\"order\" + 0.011*\"us\" + 0.010*\"time\" + 0.009*\"say\"')\n",
      "(4, '0.027*\"food\" + 0.024*\"great\" + 0.024*\"place\" + 0.018*\"good\" + 0.014*\"go\" + 0.011*\"love\" + 0.011*\"service\"')\n"
     ]
    }
   ],
   "source": [
    "NUM_TOPICS = 5\n",
    "\n",
    "#Getting 5 topics from real_train\n",
    "real_corpus = [dictionary.doc2bow(text) for text in real_train]\n",
    "perplexity1 = PerplexityMetric(corpus=real_corpus, logger='shell')\n",
    "real_model = gensim.models.ldamodel.LdaModel(real_corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=20, eval_every=1000)\n",
    "real_topics = real_model.print_topics(num_words=7)\n",
    "for topic in real_topics:\n",
    "    print(topic)\n",
    "\n",
    "#Getting 5 topics from real_test\n",
    "fake_corpus = [dictionary.doc2bow(text) for text in fake_train]\n",
    "perplexity2 = PerplexityMetric(corpus=fake_corpus, logger='shell')\n",
    "fake_model = gensim.models.ldamodel.LdaModel(fake_corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=20, eval_every=1000)\n",
    "fake_topics = fake_model.print_topics(num_words=7)\n",
    "for topic in fake_topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-d086a6abc634>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Saving all the acquired data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'E:\\Yelp\\Unfiltered Data\\YelpZip\\models and stuff\\dictionary.gensim'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'E:\\Yelp\\Unfiltered Data\\WorkingSet\\real_corpus.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'E:\\Yelp\\Unfiltered Data\\WorkingSet\\fake_corpus.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Saving all the acquired data\n",
    "dictionary.save(r'E:\\Yelp\\Unfiltered Data\\YelpZip\\models and stuff\\dictionary.gensim')\n",
    "pickle.dump(real_corpus, open(r'E:\\Yelp\\Unfiltered Data\\WorkingSet\\real_corpus.pkl', 'wb'))\n",
    "pickle.dump(fake_corpus, open(r'E:\\Yelp\\Unfiltered Data\\WorkingSet\\fake_corpus.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-16c436456302>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlda_display\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_display\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pyLDAvis\\gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m     \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pyLDAvis\\gensim.py\u001b[0m in \u001b[0;36m_extract_data\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m       \u001b[0mcorpus_csc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus2csc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_terms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m       \u001b[0mcorpus_csc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py\u001b[0m in \u001b[0;36mcorpus2csc\u001b[1;34m(corpus, num_terms, dtype, num_docs, num_nnz, printprogress)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[0mnum_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;31m# now num_docs, num_terms and num_nnz contain the correct values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \"\"\"\n\u001b[1;32m--> 492\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lda_display = pyLDAvis.gensim.prepare(real_model, real_corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.gensim.prepare(fake_model, fake_corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting a document\n",
    "new_doc = 'absolutely hated the place it suck bad waiter coffee smell bad'\n",
    "new_doc = preprocess_text(new_doc)\n",
    "new_doc_bow = dictionary.doc2bow(new_doc)\n",
    "print(new_doc_bow)\n",
    "print(real_model.get_document_topics(new_doc_bow))\n",
    "print(fake_model.get_document_topics(new_doc_bow))\n",
    "print(real_model.get_document_topics(new_doc_bow)[3][1])\n",
    "max(real_model.get_document_topics(new_doc_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we create a function to evaluate the 400 test sets.\n",
    "### We joing the test sets in order: real_test, fake_test\n",
    "### We create y_actual with first 200 elements as 1 and next 200 as -1\n",
    "### We pass each individual text through all the 10 topics(5 real then 5 fake).\n",
    "### We find the best fit and estimate from the first 5 models(real) and then the best fit and estimate from the next 5 models(fake).\n",
    "### We compare the fits and accordingly assign the prediction to our text. \n",
    "### Append the prediction(1,-1) to y_pred list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test_set: 200\n",
      "y_actual: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "#Creating the test set\n",
    "test_set = []\n",
    "\n",
    "pbar = tqdm_notebook(total=X_test.shape[0])\n",
    "for text in X_test:\n",
    "    tokens = preprocess_text(str(text))\n",
    "    test_set.append(tokens)\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "\n",
    "y_actual = []\n",
    "for i in Y_test:\n",
    "    if i == 1:\n",
    "        y_actual.append(1)\n",
    "    else:\n",
    "        y_actual.append(-1)\n",
    "        \n",
    "print(\"Size of test_set:\", len(test_set[1]))\n",
    "print(\"y_actual:\", y_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions based on topics\n",
    "y_pred = []\n",
    "\n",
    "for test_text in test_set:\n",
    "    test_doc_bow = dictionary.doc2bow(test_text)\n",
    "    real_scores = []\n",
    "    fake_scores = []\n",
    "    for i in real_model.get_document_topics(test_doc_bow):\n",
    "        #real_model.get_document_topics(new_doc_bow)[0][1] -> Fit value of doc on topic 0\n",
    "        real_scores.append(i[1])\n",
    "    for i in fake_model.get_document_topics(test_doc_bow):\n",
    "        fake_scores.append(i[1])\n",
    "        \n",
    "    #Best Fit on real model\n",
    "    real_fit = max(real_scores)\n",
    "    #Best Fit on fake model\n",
    "    fake_fit = max(fake_scores)\n",
    "    \n",
    "    if real_fit >= fake_fit:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(-1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 52.25 %\n",
      "Real prediction accuracy: 43.0\n",
      "Fake prediction accuracy: 61.5\n"
     ]
    }
   ],
   "source": [
    "#Evaluating\n",
    "real_correct = 0\n",
    "fake_correct = 0\n",
    "\n",
    "for i in Y_test.shape[0]:\n",
    "    if y_pred[i] == y_actual[i]:\n",
    "        if i<200:\n",
    "            real_correct = real_correct + 1\n",
    "        else:\n",
    "            fake_correct = fake_correct + 1\n",
    "        \n",
    "acc = ((real_correct+fake_correct)/400)*100\n",
    "print(\"Overall Accuracy:\", acc,\"%\")\n",
    "print(\"Real prediction accuracy:\", ((real_correct/200)*100))\n",
    "print(\"Fake prediction accuracy:\", ((fake_correct/200)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
