{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import keras\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Sequential\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, TimeDistributed, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras.layers import Input\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>real</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5044</td>\n",
       "      <td>0</td>\n",
       "      <td>drink be bad the hot chocolate be water down a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2014-11-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5045</td>\n",
       "      <td>0</td>\n",
       "      <td>this be the bad experience ive ever have a cas...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2014-09-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5046</td>\n",
       "      <td>0</td>\n",
       "      <td>this be locate on the site of the old spruce s...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2013-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5047</td>\n",
       "      <td>0</td>\n",
       "      <td>i enjoy coffee and breakfast twice at toast du...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2014-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5048</td>\n",
       "      <td>0</td>\n",
       "      <td>i love toast the food choice be fantastic i lo...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2014-08-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  user  item                                             review  \\\n",
       "0      0  5044     0  drink be bad the hot chocolate be water down a...   \n",
       "1      1  5045     0  this be the bad experience ive ever have a cas...   \n",
       "2      2  5046     0  this be locate on the site of the old spruce s...   \n",
       "3      3  5047     0  i enjoy coffee and breakfast twice at toast du...   \n",
       "4      4  5048     0  i love toast the food choice be fantastic i lo...   \n",
       "\n",
       "   rating  real        date  \n",
       "0     1.0    -1  2014-11-16  \n",
       "1     1.0    -1  2014-09-08  \n",
       "2     3.0    -1  2013-10-06  \n",
       "3     5.0    -1  2014-11-30  \n",
       "4     5.0    -1  2014-08-28  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save checkpoint\n",
    "# data.to_csv(r\"E:\\Yelp\\Unfiltered Data\\YelpZip\\Customs\\checkpoint\", header=True, index=False)\n",
    "\n",
    "#Use if kernel crashes\n",
    "data = pd.read_csv(r\"E:\\Yelp\\Unfiltered Data\\YelpZip\\Customs\\checkpoint\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review'] = data['review'].apply(lambda s: str(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 for good\n",
    "#0 for bad\n",
    "def binarize(n):\n",
    "    if n==-1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data['real'] = data['real'].apply(lambda s: binarize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['review'], data['real'], test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>528019</td>\n",
       "      <td>528019</td>\n",
       "      <td>528019</td>\n",
       "      <td>528019</td>\n",
       "      <td>528019</td>\n",
       "      <td>528019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80439</td>\n",
       "      <td>80439</td>\n",
       "      <td>80439</td>\n",
       "      <td>80439</td>\n",
       "      <td>80439</td>\n",
       "      <td>80439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index    user    item  review  rating    date\n",
       "real                                                \n",
       "0     528019  528019  528019  528019  528019  528019\n",
       "1      80439   80439   80439   80439   80439   80439"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('real').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words= 2000)\n",
    "tokenizer.fit_on_texts(data['review'])\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "data1 = pad_sequences(sequences, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 32)           64000     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 100, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 50, 32)            128       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 50, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 25, 32)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 25, 32)            128       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               205056    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 310,081\n",
      "Trainable params: 309,185\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# inputs = Input(shape=[100])\n",
    "# layers = Embedding(2000,32,input_length=100)(inputs)\n",
    "# layers = LSTM(128, recurrent_dropout=0.2)(layers)\n",
    "# layers = Dense(1)(layers)\n",
    "# layers = Activation('sigmoid')(layers)\n",
    "# model = Model(inputs=inputs,outputs=layers)\n",
    "# model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "\n",
    "# def create_conv_model():\n",
    "#     model_conv = Sequential()\n",
    "#     model_conv.add(Embedding(vocabulary_size, 100, input_length=50))\n",
    "#     model_conv.add(Dropout(0.2))\n",
    "#     model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "#     model_conv.add(MaxPooling1D(pool_size=4))\n",
    "#     model_conv.add(LSTM(100))\n",
    "#     model_conv.add(Dense(1, activation='sigmoid'))\n",
    "#     model_conv.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     return model_conv\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(2000, 32, input_length=100))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194000/425920 [============>.................] - ETA: 3:09:59 - loss: 0.9111 - acc: 0.48 - ETA: 2:59:31 - loss: 0.8804 - acc: 0.49 - ETA: 3:01:22 - loss: 0.8737 - acc: 0.50 - ETA: 2:59:15 - loss: 0.8628 - acc: 0.50 - ETA: 2:57:48 - loss: 0.8462 - acc: 0.51 - ETA: 2:56:11 - loss: 0.8370 - acc: 0.51 - ETA: 2:56:01 - loss: 0.8289 - acc: 0.52 - ETA: 2:54:24 - loss: 0.8273 - acc: 0.52 - ETA: 2:54:59 - loss: 0.8194 - acc: 0.53 - ETA: 2:53:28 - loss: 0.8155 - acc: 0.53 - ETA: 2:53:28 - loss: 0.8101 - acc: 0.53 - ETA: 2:53:22 - loss: 0.8042 - acc: 0.53 - ETA: 2:52:26 - loss: 0.7976 - acc: 0.54 - ETA: 2:51:17 - loss: 0.7931 - acc: 0.54 - ETA: 2:51:02 - loss: 0.7898 - acc: 0.54 - ETA: 2:51:15 - loss: 0.7848 - acc: 0.54 - ETA: 2:50:58 - loss: 0.7814 - acc: 0.54 - ETA: 2:50:49 - loss: 0.7781 - acc: 0.54 - ETA: 2:51:57 - loss: 0.7740 - acc: 0.55 - ETA: 2:52:28 - loss: 0.7707 - acc: 0.55 - ETA: 2:51:53 - loss: 0.7671 - acc: 0.55 - ETA: 2:51:33 - loss: 0.7641 - acc: 0.55 - ETA: 2:50:55 - loss: 0.7605 - acc: 0.56 - ETA: 2:51:07 - loss: 0.7568 - acc: 0.56 - ETA: 2:50:40 - loss: 0.7534 - acc: 0.56 - ETA: 2:49:56 - loss: 0.7501 - acc: 0.56 - ETA: 2:49:14 - loss: 0.7466 - acc: 0.57 - ETA: 2:49:02 - loss: 0.7426 - acc: 0.57 - ETA: 2:49:07 - loss: 0.7387 - acc: 0.57 - ETA: 2:48:24 - loss: 0.7360 - acc: 0.57 - ETA: 2:47:55 - loss: 0.7331 - acc: 0.57 - ETA: 2:47:24 - loss: 0.7303 - acc: 0.58 - ETA: 2:46:37 - loss: 0.7281 - acc: 0.58 - ETA: 2:45:36 - loss: 0.7246 - acc: 0.58 - ETA: 2:45:21 - loss: 0.7219 - acc: 0.58 - ETA: 2:45:10 - loss: 0.7195 - acc: 0.58 - ETA: 2:44:20 - loss: 0.7172 - acc: 0.59 - ETA: 2:44:11 - loss: 0.7145 - acc: 0.59 - ETA: 2:43:32 - loss: 0.7120 - acc: 0.59 - ETA: 2:42:53 - loss: 0.7101 - acc: 0.59 - ETA: 2:43:04 - loss: 0.7075 - acc: 0.59 - ETA: 2:42:32 - loss: 0.7050 - acc: 0.60 - ETA: 2:42:16 - loss: 0.7027 - acc: 0.60 - ETA: 2:42:08 - loss: 0.7001 - acc: 0.60 - ETA: 2:41:52 - loss: 0.6978 - acc: 0.60 - ETA: 2:41:29 - loss: 0.6955 - acc: 0.60 - ETA: 2:41:27 - loss: 0.6932 - acc: 0.60 - ETA: 2:41:00 - loss: 0.6911 - acc: 0.61 - ETA: 2:40:30 - loss: 0.6896 - acc: 0.61 - ETA: 2:40:03 - loss: 0.6874 - acc: 0.61 - ETA: 2:39:40 - loss: 0.6848 - acc: 0.61 - ETA: 2:38:59 - loss: 0.6828 - acc: 0.61 - ETA: 2:38:26 - loss: 0.6807 - acc: 0.61 - ETA: 2:37:49 - loss: 0.6785 - acc: 0.62 - ETA: 2:37:37 - loss: 0.6768 - acc: 0.62 - ETA: 2:37:18 - loss: 0.6752 - acc: 0.62 - ETA: 2:37:01 - loss: 0.6730 - acc: 0.62 - ETA: 2:36:28 - loss: 0.6714 - acc: 0.62 - ETA: 2:35:48 - loss: 0.6697 - acc: 0.62 - ETA: 2:35:13 - loss: 0.6679 - acc: 0.63 - ETA: 2:34:59 - loss: 0.6661 - acc: 0.63 - ETA: 2:34:30 - loss: 0.6642 - acc: 0.63 - ETA: 2:34:03 - loss: 0.6621 - acc: 0.63 - ETA: 2:33:32 - loss: 0.6604 - acc: 0.63 - ETA: 2:32:56 - loss: 0.6585 - acc: 0.63 - ETA: 2:32:28 - loss: 0.6567 - acc: 0.64 - ETA: 2:31:55 - loss: 0.6550 - acc: 0.64 - ETA: 2:31:23 - loss: 0.6528 - acc: 0.64 - ETA: 2:30:57 - loss: 0.6510 - acc: 0.64 - ETA: 2:30:36 - loss: 0.6493 - acc: 0.64 - ETA: 2:30:11 - loss: 0.6477 - acc: 0.64 - ETA: 2:29:38 - loss: 0.6458 - acc: 0.65 - ETA: 2:29:20 - loss: 0.6443 - acc: 0.65 - ETA: 2:28:41 - loss: 0.6424 - acc: 0.65 - ETA: 2:28:09 - loss: 0.6404 - acc: 0.65 - ETA: 2:27:38 - loss: 0.6387 - acc: 0.65 - ETA: 2:27:02 - loss: 0.6373 - acc: 0.65 - ETA: 2:26:29 - loss: 0.6358 - acc: 0.65 - ETA: 2:25:58 - loss: 0.6342 - acc: 0.66 - ETA: 2:25:34 - loss: 0.6327 - acc: 0.66 - ETA: 2:25:12 - loss: 0.6309 - acc: 0.66 - ETA: 2:24:51 - loss: 0.6292 - acc: 0.66 - ETA: 2:24:27 - loss: 0.6277 - acc: 0.66 - ETA: 2:23:55 - loss: 0.6262 - acc: 0.66 - ETA: 2:23:26 - loss: 0.6245 - acc: 0.67 - ETA: 2:23:02 - loss: 0.6231 - acc: 0.67 - ETA: 2:22:35 - loss: 0.6214 - acc: 0.67 - ETA: 2:22:18 - loss: 0.6198 - acc: 0.67 - ETA: 2:21:43 - loss: 0.6182 - acc: 0.67 - ETA: 2:21:18 - loss: 0.6170 - acc: 0.67 - ETA: 2:20:55 - loss: 0.6157 - acc: 0.67 - ETA: 2:20:26 - loss: 0.6142 - acc: 0.68 - ETA: 2:20:06 - loss: 0.6127 - acc: 0.68 - ETA: 2:19:36 - loss: 0.6111 - acc: 0.68 - ETA: 2:19:12 - loss: 0.6094 - acc: 0.68 - ETA: 2:18:44 - loss: 0.6080 - acc: 0.68 - ETA: 2:18:14 - loss: 0.6066 - acc: 0.68 - ETA: 2:17:49 - loss: 0.6052 - acc: 0.68 - ETA: 2:17:27 - loss: 0.6039 - acc: 0.68 - ETA: 2:17:01 - loss: 0.6025 - acc: 0.69 - ETA: 2:16:41 - loss: 0.6011 - acc: 0.69 - ETA: 2:16:11 - loss: 0.5997 - acc: 0.69 - ETA: 2:15:37 - loss: 0.5981 - acc: 0.69 - ETA: 2:15:06 - loss: 0.5966 - acc: 0.69 - ETA: 2:14:37 - loss: 0.5953 - acc: 0.69 - ETA: 2:14:11 - loss: 0.5941 - acc: 0.69 - ETA: 2:13:46 - loss: 0.5931 - acc: 0.69 - ETA: 2:13:29 - loss: 0.5920 - acc: 0.70 - ETA: 2:13:06 - loss: 0.5904 - acc: 0.70 - ETA: 2:12:42 - loss: 0.5892 - acc: 0.70 - ETA: 2:12:24 - loss: 0.5881 - acc: 0.70 - ETA: 2:11:54 - loss: 0.5869 - acc: 0.70 - ETA: 2:11:23 - loss: 0.5858 - acc: 0.70 - ETA: 2:10:54 - loss: 0.5846 - acc: 0.70 - ETA: 2:10:26 - loss: 0.5835 - acc: 0.70 - ETA: 2:09:56 - loss: 0.5823 - acc: 0.71 - ETA: 2:09:29 - loss: 0.5812 - acc: 0.71 - ETA: 2:09:05 - loss: 0.5801 - acc: 0.71 - ETA: 2:08:43 - loss: 0.5791 - acc: 0.71 - ETA: 2:08:14 - loss: 0.5780 - acc: 0.71 - ETA: 2:07:44 - loss: 0.5769 - acc: 0.71 - ETA: 2:07:22 - loss: 0.5756 - acc: 0.71 - ETA: 2:06:48 - loss: 0.5743 - acc: 0.71 - ETA: 2:06:28 - loss: 0.5730 - acc: 0.71 - ETA: 2:06:01 - loss: 0.5718 - acc: 0.71 - ETA: 2:05:42 - loss: 0.5707 - acc: 0.72 - ETA: 2:05:20 - loss: 0.5696 - acc: 0.72 - ETA: 2:05:04 - loss: 0.5685 - acc: 0.72 - ETA: 2:04:35 - loss: 0.5674 - acc: 0.72 - ETA: 2:04:06 - loss: 0.5664 - acc: 0.72 - ETA: 2:03:39 - loss: 0.5652 - acc: 0.72 - ETA: 2:03:12 - loss: 0.5641 - acc: 0.72 - ETA: 2:02:47 - loss: 0.5635 - acc: 0.72 - ETA: 2:02:21 - loss: 0.5625 - acc: 0.72 - ETA: 2:01:54 - loss: 0.5612 - acc: 0.72 - ETA: 2:01:27 - loss: 0.5605 - acc: 0.73 - ETA: 2:01:03 - loss: 0.5595 - acc: 0.73 - ETA: 2:00:38 - loss: 0.5586 - acc: 0.73 - ETA: 2:00:12 - loss: 0.5576 - acc: 0.73 - ETA: 1:59:45 - loss: 0.5564 - acc: 0.73 - ETA: 1:59:19 - loss: 0.5554 - acc: 0.73 - ETA: 1:58:55 - loss: 0.5546 - acc: 0.73 - ETA: 1:58:32 - loss: 0.5536 - acc: 0.73 - ETA: 1:58:10 - loss: 0.5527 - acc: 0.73 - ETA: 1:57:44 - loss: 0.5519 - acc: 0.73 - ETA: 1:57:18 - loss: 0.5513 - acc: 0.73 - ETA: 1:56:56 - loss: 0.5503 - acc: 0.73 - ETA: 1:56:30 - loss: 0.5494 - acc: 0.74 - ETA: 1:56:02 - loss: 0.5488 - acc: 0.74 - ETA: 1:55:36 - loss: 0.5478 - acc: 0.74 - ETA: 1:55:09 - loss: 0.5470 - acc: 0.74 - ETA: 1:54:41 - loss: 0.5461 - acc: 0.74 - ETA: 1:54:17 - loss: 0.5452 - acc: 0.74 - ETA: 1:53:50 - loss: 0.5446 - acc: 0.74 - ETA: 1:53:23 - loss: 0.5440 - acc: 0.74 - ETA: 1:52:57 - loss: 0.5433 - acc: 0.74 - ETA: 1:52:30 - loss: 0.5424 - acc: 0.74 - ETA: 1:52:10 - loss: 0.5413 - acc: 0.74 - ETA: 1:51:48 - loss: 0.5406 - acc: 0.74 - ETA: 1:51:28 - loss: 0.5399 - acc: 0.74 - ETA: 1:51:09 - loss: 0.5389 - acc: 0.75 - ETA: 1:50:47 - loss: 0.5381 - acc: 0.75 - ETA: 1:50:24 - loss: 0.5374 - acc: 0.75 - ETA: 1:50:03 - loss: 0.5364 - acc: 0.75 - ETA: 1:49:43 - loss: 0.5354 - acc: 0.75 - ETA: 1:49:19 - loss: 0.5349 - acc: 0.75 - ETA: 1:48:59 - loss: 0.5340 - acc: 0.75 - ETA: 1:48:33 - loss: 0.5335 - acc: 0.75 - ETA: 1:48:10 - loss: 0.5328 - acc: 0.75 - ETA: 1:47:42 - loss: 0.5321 - acc: 0.75 - ETA: 1:47:23 - loss: 0.5313 - acc: 0.75 - ETA: 1:46:58 - loss: 0.5305 - acc: 0.75 - ETA: 1:46:36 - loss: 0.5299 - acc: 0.75 - ETA: 1:46:09 - loss: 0.5294 - acc: 0.75 - ETA: 1:45:42 - loss: 0.5289 - acc: 0.75 - ETA: 1:45:17 - loss: 0.5283 - acc: 0.76 - ETA: 1:44:50 - loss: 0.5275 - acc: 0.76 - ETA: 1:44:24 - loss: 0.5269 - acc: 0.76 - ETA: 1:44:00 - loss: 0.5262 - acc: 0.76 - ETA: 1:43:34 - loss: 0.5257 - acc: 0.76 - ETA: 1:43:09 - loss: 0.5251 - acc: 0.76 - ETA: 1:42:47 - loss: 0.5244 - acc: 0.76 - ETA: 1:42:23 - loss: 0.5240 - acc: 0.76 - ETA: 1:41:58 - loss: 0.5234 - acc: 0.76 - ETA: 1:41:33 - loss: 0.5225 - acc: 0.76 - ETA: 1:41:08 - loss: 0.5218 - acc: 0.76 - ETA: 1:40:42 - loss: 0.5212 - acc: 0.76 - ETA: 1:40:14 - loss: 0.5207 - acc: 0.76 - ETA: 1:39:48 - loss: 0.5200 - acc: 0.76 - ETA: 1:39:21 - loss: 0.5197 - acc: 0.76 - ETA: 1:38:58 - loss: 0.5192 - acc: 0.76 - ETA: 1:38:32 - loss: 0.5185 - acc: 0.76 - ETA: 1:38:06 - loss: 0.5179 - acc: 0.76 - ETA: 1:37:47 - loss: 0.5174 - acc: 0.76394000/425920 [==========================>...] - ETA: 1:37:20 - loss: 0.5168 - acc: 0.77 - ETA: 1:36:57 - loss: 0.5164 - acc: 0.77 - ETA: 1:36:34 - loss: 0.5157 - acc: 0.77 - ETA: 1:36:08 - loss: 0.5151 - acc: 0.77 - ETA: 1:35:44 - loss: 0.5143 - acc: 0.77 - ETA: 1:35:22 - loss: 0.5140 - acc: 0.77 - ETA: 1:34:57 - loss: 0.5133 - acc: 0.77 - ETA: 1:34:30 - loss: 0.5127 - acc: 0.77 - ETA: 1:34:05 - loss: 0.5121 - acc: 0.77 - ETA: 1:33:41 - loss: 0.5116 - acc: 0.77 - ETA: 1:33:18 - loss: 0.5112 - acc: 0.77 - ETA: 1:32:52 - loss: 0.5107 - acc: 0.77 - ETA: 1:32:28 - loss: 0.5102 - acc: 0.77 - ETA: 1:32:00 - loss: 0.5095 - acc: 0.77 - ETA: 1:31:36 - loss: 0.5092 - acc: 0.77 - ETA: 1:31:12 - loss: 0.5088 - acc: 0.77 - ETA: 1:30:47 - loss: 0.5082 - acc: 0.77 - ETA: 1:30:23 - loss: 0.5076 - acc: 0.77 - ETA: 1:29:58 - loss: 0.5072 - acc: 0.77 - ETA: 1:29:31 - loss: 0.5068 - acc: 0.77 - ETA: 1:29:04 - loss: 0.5064 - acc: 0.77 - ETA: 1:28:39 - loss: 0.5058 - acc: 0.77 - ETA: 1:28:14 - loss: 0.5053 - acc: 0.77 - ETA: 1:27:47 - loss: 0.5047 - acc: 0.78 - ETA: 1:27:22 - loss: 0.5042 - acc: 0.78 - ETA: 1:26:56 - loss: 0.5037 - acc: 0.78 - ETA: 1:26:33 - loss: 0.5032 - acc: 0.78 - ETA: 1:26:07 - loss: 0.5028 - acc: 0.78 - ETA: 1:25:44 - loss: 0.5022 - acc: 0.78 - ETA: 1:25:19 - loss: 0.5018 - acc: 0.78 - ETA: 1:24:53 - loss: 0.5015 - acc: 0.78 - ETA: 1:24:27 - loss: 0.5011 - acc: 0.78 - ETA: 1:24:02 - loss: 0.5007 - acc: 0.78 - ETA: 1:23:35 - loss: 0.5000 - acc: 0.78 - ETA: 1:23:11 - loss: 0.4996 - acc: 0.78 - ETA: 1:22:44 - loss: 0.4992 - acc: 0.78 - ETA: 1:22:18 - loss: 0.4989 - acc: 0.78 - ETA: 1:21:52 - loss: 0.4984 - acc: 0.78 - ETA: 1:21:24 - loss: 0.4979 - acc: 0.78 - ETA: 1:20:58 - loss: 0.4975 - acc: 0.78 - ETA: 1:20:32 - loss: 0.4969 - acc: 0.78 - ETA: 1:20:07 - loss: 0.4964 - acc: 0.78 - ETA: 1:19:42 - loss: 0.4960 - acc: 0.78 - ETA: 1:19:17 - loss: 0.4957 - acc: 0.78 - ETA: 1:18:51 - loss: 0.4953 - acc: 0.78 - ETA: 1:18:24 - loss: 0.4949 - acc: 0.78 - ETA: 1:18:01 - loss: 0.4945 - acc: 0.78 - ETA: 1:17:36 - loss: 0.4940 - acc: 0.78 - ETA: 1:17:10 - loss: 0.4937 - acc: 0.78 - ETA: 1:16:45 - loss: 0.4932 - acc: 0.78 - ETA: 1:16:19 - loss: 0.4928 - acc: 0.78 - ETA: 1:15:55 - loss: 0.4924 - acc: 0.79 - ETA: 1:15:32 - loss: 0.4922 - acc: 0.79 - ETA: 1:15:08 - loss: 0.4919 - acc: 0.79 - ETA: 1:14:45 - loss: 0.4916 - acc: 0.79 - ETA: 1:14:19 - loss: 0.4911 - acc: 0.79 - ETA: 1:13:54 - loss: 0.4907 - acc: 0.79 - ETA: 1:13:31 - loss: 0.4904 - acc: 0.79 - ETA: 1:13:06 - loss: 0.4899 - acc: 0.79 - ETA: 1:12:41 - loss: 0.4895 - acc: 0.79 - ETA: 1:12:16 - loss: 0.4891 - acc: 0.79 - ETA: 1:11:51 - loss: 0.4888 - acc: 0.79 - ETA: 1:11:25 - loss: 0.4883 - acc: 0.79 - ETA: 1:11:01 - loss: 0.4878 - acc: 0.79 - ETA: 1:10:36 - loss: 0.4875 - acc: 0.79 - ETA: 1:10:11 - loss: 0.4871 - acc: 0.79 - ETA: 1:09:44 - loss: 0.4866 - acc: 0.79 - ETA: 1:09:18 - loss: 0.4862 - acc: 0.79 - ETA: 1:08:52 - loss: 0.4858 - acc: 0.79 - ETA: 1:08:25 - loss: 0.4855 - acc: 0.79 - ETA: 1:08:00 - loss: 0.4851 - acc: 0.79 - ETA: 1:07:34 - loss: 0.4846 - acc: 0.79 - ETA: 1:07:09 - loss: 0.4843 - acc: 0.79 - ETA: 1:06:43 - loss: 0.4841 - acc: 0.79 - ETA: 1:06:18 - loss: 0.4837 - acc: 0.79 - ETA: 1:05:53 - loss: 0.4833 - acc: 0.79 - ETA: 1:05:26 - loss: 0.4829 - acc: 0.79 - ETA: 1:05:00 - loss: 0.4825 - acc: 0.79 - ETA: 1:04:35 - loss: 0.4821 - acc: 0.79 - ETA: 1:04:10 - loss: 0.4818 - acc: 0.79 - ETA: 1:03:45 - loss: 0.4814 - acc: 0.79 - ETA: 1:03:19 - loss: 0.4811 - acc: 0.79 - ETA: 1:02:54 - loss: 0.4807 - acc: 0.79 - ETA: 1:02:28 - loss: 0.4803 - acc: 0.79 - ETA: 1:02:03 - loss: 0.4798 - acc: 0.79 - ETA: 1:01:38 - loss: 0.4794 - acc: 0.79 - ETA: 1:01:13 - loss: 0.4791 - acc: 0.79 - ETA: 1:00:48 - loss: 0.4786 - acc: 0.80 - ETA: 1:00:24 - loss: 0.4782 - acc: 0.80 - ETA: 59:59 - loss: 0.4779 - acc: 0.8007 - ETA: 59:34 - loss: 0.4775 - acc: 0.80 - ETA: 59:09 - loss: 0.4772 - acc: 0.80 - ETA: 58:45 - loss: 0.4769 - acc: 0.80 - ETA: 58:19 - loss: 0.4766 - acc: 0.80 - ETA: 57:53 - loss: 0.4763 - acc: 0.80 - ETA: 57:27 - loss: 0.4760 - acc: 0.80 - ETA: 57:03 - loss: 0.4758 - acc: 0.80 - ETA: 56:38 - loss: 0.4754 - acc: 0.80 - ETA: 56:11 - loss: 0.4752 - acc: 0.80 - ETA: 55:46 - loss: 0.4748 - acc: 0.80 - ETA: 55:20 - loss: 0.4744 - acc: 0.80 - ETA: 54:54 - loss: 0.4740 - acc: 0.80 - ETA: 54:28 - loss: 0.4737 - acc: 0.80 - ETA: 54:04 - loss: 0.4734 - acc: 0.80 - ETA: 53:39 - loss: 0.4732 - acc: 0.80 - ETA: 53:14 - loss: 0.4728 - acc: 0.80 - ETA: 52:50 - loss: 0.4727 - acc: 0.80 - ETA: 52:24 - loss: 0.4723 - acc: 0.80 - ETA: 51:59 - loss: 0.4719 - acc: 0.80 - ETA: 51:33 - loss: 0.4716 - acc: 0.80 - ETA: 51:08 - loss: 0.4714 - acc: 0.80 - ETA: 50:42 - loss: 0.4710 - acc: 0.80 - ETA: 50:16 - loss: 0.4706 - acc: 0.80 - ETA: 49:51 - loss: 0.4703 - acc: 0.80 - ETA: 49:25 - loss: 0.4700 - acc: 0.80 - ETA: 48:59 - loss: 0.4698 - acc: 0.80 - ETA: 48:34 - loss: 0.4695 - acc: 0.80 - ETA: 48:09 - loss: 0.4693 - acc: 0.80 - ETA: 47:43 - loss: 0.4690 - acc: 0.80 - ETA: 47:18 - loss: 0.4686 - acc: 0.80 - ETA: 46:52 - loss: 0.4682 - acc: 0.80 - ETA: 46:27 - loss: 0.4678 - acc: 0.80 - ETA: 46:01 - loss: 0.4676 - acc: 0.80 - ETA: 45:36 - loss: 0.4673 - acc: 0.80 - ETA: 45:10 - loss: 0.4670 - acc: 0.80 - ETA: 44:44 - loss: 0.4667 - acc: 0.80 - ETA: 44:19 - loss: 0.4664 - acc: 0.80 - ETA: 43:54 - loss: 0.4661 - acc: 0.80 - ETA: 43:29 - loss: 0.4658 - acc: 0.80 - ETA: 43:04 - loss: 0.4657 - acc: 0.80 - ETA: 42:39 - loss: 0.4653 - acc: 0.80 - ETA: 42:14 - loss: 0.4651 - acc: 0.80 - ETA: 41:49 - loss: 0.4650 - acc: 0.80 - ETA: 41:23 - loss: 0.4647 - acc: 0.80 - ETA: 40:59 - loss: 0.4644 - acc: 0.80 - ETA: 40:33 - loss: 0.4641 - acc: 0.81 - ETA: 40:08 - loss: 0.4638 - acc: 0.81 - ETA: 39:43 - loss: 0.4635 - acc: 0.81 - ETA: 39:18 - loss: 0.4633 - acc: 0.81 - ETA: 38:53 - loss: 0.4630 - acc: 0.81 - ETA: 38:27 - loss: 0.4626 - acc: 0.81 - ETA: 38:02 - loss: 0.4623 - acc: 0.81 - ETA: 37:37 - loss: 0.4621 - acc: 0.81 - ETA: 37:11 - loss: 0.4618 - acc: 0.81 - ETA: 36:46 - loss: 0.4615 - acc: 0.81 - ETA: 36:20 - loss: 0.4612 - acc: 0.81 - ETA: 35:56 - loss: 0.4610 - acc: 0.81 - ETA: 35:30 - loss: 0.4607 - acc: 0.81 - ETA: 35:05 - loss: 0.4604 - acc: 0.81 - ETA: 34:40 - loss: 0.4601 - acc: 0.81 - ETA: 34:14 - loss: 0.4598 - acc: 0.81 - ETA: 33:48 - loss: 0.4597 - acc: 0.81 - ETA: 33:23 - loss: 0.4594 - acc: 0.81 - ETA: 32:57 - loss: 0.4590 - acc: 0.81 - ETA: 32:32 - loss: 0.4587 - acc: 0.81 - ETA: 32:07 - loss: 0.4585 - acc: 0.81 - ETA: 31:42 - loss: 0.4583 - acc: 0.81 - ETA: 31:17 - loss: 0.4580 - acc: 0.81 - ETA: 30:52 - loss: 0.4578 - acc: 0.81 - ETA: 30:26 - loss: 0.4576 - acc: 0.81 - ETA: 30:01 - loss: 0.4573 - acc: 0.81 - ETA: 29:36 - loss: 0.4571 - acc: 0.81 - ETA: 29:11 - loss: 0.4568 - acc: 0.81 - ETA: 28:46 - loss: 0.4566 - acc: 0.81 - ETA: 28:20 - loss: 0.4565 - acc: 0.81 - ETA: 27:55 - loss: 0.4562 - acc: 0.81 - ETA: 27:30 - loss: 0.4560 - acc: 0.81 - ETA: 27:05 - loss: 0.4557 - acc: 0.81 - ETA: 26:39 - loss: 0.4555 - acc: 0.81 - ETA: 26:14 - loss: 0.4552 - acc: 0.81 - ETA: 25:48 - loss: 0.4549 - acc: 0.81 - ETA: 25:23 - loss: 0.4546 - acc: 0.81 - ETA: 24:57 - loss: 0.4544 - acc: 0.81 - ETA: 24:31 - loss: 0.4542 - acc: 0.81 - ETA: 24:06 - loss: 0.4538 - acc: 0.81 - ETA: 23:40 - loss: 0.4536 - acc: 0.81 - ETA: 23:15 - loss: 0.4534 - acc: 0.81 - ETA: 22:49 - loss: 0.4531 - acc: 0.81 - ETA: 22:24 - loss: 0.4528 - acc: 0.81 - ETA: 21:58 - loss: 0.4526 - acc: 0.81 - ETA: 21:33 - loss: 0.4524 - acc: 0.81 - ETA: 21:08 - loss: 0.4521 - acc: 0.81 - ETA: 20:42 - loss: 0.4518 - acc: 0.81 - ETA: 20:17 - loss: 0.4515 - acc: 0.81 - ETA: 19:52 - loss: 0.4512 - acc: 0.81 - ETA: 19:27 - loss: 0.4509 - acc: 0.81 - ETA: 19:01 - loss: 0.4507 - acc: 0.81 - ETA: 18:36 - loss: 0.4505 - acc: 0.81 - ETA: 18:10 - loss: 0.4504 - acc: 0.81 - ETA: 17:45 - loss: 0.4500 - acc: 0.81 - ETA: 17:19 - loss: 0.4499 - acc: 0.81 - ETA: 16:54 - loss: 0.4497 - acc: 0.81 - ETA: 16:28 - loss: 0.4495 - acc: 0.81 - ETA: 16:03 - loss: 0.4493 - acc: 0.81 - ETA: 15:38 - loss: 0.4490 - acc: 0.81 - ETA: 15:12 - loss: 0.4489 - acc: 0.81 - ETA: 14:47 - loss: 0.4487 - acc: 0.81 - ETA: 14:21 - loss: 0.4484 - acc: 0.81 - ETA: 13:56 - loss: 0.4482 - acc: 0.81 - ETA: 13:30 - loss: 0.4480 - acc: 0.8198"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425920/425920 [==============================] - ETA: 13:05 - loss: 0.4479 - acc: 0.81 - ETA: 12:39 - loss: 0.4477 - acc: 0.82 - ETA: 12:14 - loss: 0.4474 - acc: 0.82 - ETA: 11:49 - loss: 0.4473 - acc: 0.82 - ETA: 11:23 - loss: 0.4471 - acc: 0.82 - ETA: 10:58 - loss: 0.4469 - acc: 0.82 - ETA: 10:33 - loss: 0.4468 - acc: 0.82 - ETA: 10:07 - loss: 0.4466 - acc: 0.82 - ETA: 9:42 - loss: 0.4463 - acc: 0.8208 - ETA: 9:16 - loss: 0.4462 - acc: 0.820 - ETA: 8:51 - loss: 0.4460 - acc: 0.821 - ETA: 8:25 - loss: 0.4457 - acc: 0.821 - ETA: 8:00 - loss: 0.4456 - acc: 0.821 - ETA: 7:35 - loss: 0.4455 - acc: 0.821 - ETA: 7:09 - loss: 0.4452 - acc: 0.821 - ETA: 6:44 - loss: 0.4450 - acc: 0.821 - ETA: 6:18 - loss: 0.4448 - acc: 0.821 - ETA: 5:53 - loss: 0.4447 - acc: 0.821 - ETA: 5:28 - loss: 0.4446 - acc: 0.821 - ETA: 5:02 - loss: 0.4444 - acc: 0.821 - ETA: 4:37 - loss: 0.4443 - acc: 0.822 - ETA: 4:11 - loss: 0.4441 - acc: 0.822 - ETA: 3:46 - loss: 0.4439 - acc: 0.822 - ETA: 3:21 - loss: 0.4438 - acc: 0.822 - ETA: 2:55 - loss: 0.4435 - acc: 0.822 - ETA: 2:30 - loss: 0.4435 - acc: 0.822 - ETA: 2:04 - loss: 0.4432 - acc: 0.822 - ETA: 1:39 - loss: 0.4431 - acc: 0.822 - ETA: 1:14 - loss: 0.4429 - acc: 0.822 - ETA: 48s - loss: 0.4427 - acc: 0.823 - ETA: 23s - loss: 0.4425 - acc: 0.82 - 10813s 25ms/step - loss: 0.4422 - acc: 0.8232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2033da82668>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data1,y_train,batch_size=1000,epochs=1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = pad_sequences(test_sequences,maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accr = model.evaluate(test_sequences_matrix,y_test)\n",
    "y_pred = model.predict(test_sequences_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93    158422\n",
      "          1       0.00      0.00      0.00     24116\n",
      "\n",
      "avg / total       0.75      0.87      0.81    182538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tst = []\n",
    "for i in y_pred:\n",
    "    if i >= 0.60:\n",
    "        tst.append(1)\n",
    "    else:\n",
    "        tst.append(0)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182538/182538 [==============================] - ETA: 14:5 - ETA: 1:0 - ETA: 42s - ETA: 34 - ETA: 29 - ETA: 26 - ETA: 24 - ETA: 23 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 17s 91us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_sequences_matrix, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.78850431149635 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", results[1]*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14800052828693527"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
